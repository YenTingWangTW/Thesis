{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install virtualenv\n",
    "# !virtualenv my_env\n",
    "# !python -m ipykernel install --user --name=my_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip3 install torch==1.10.1+cu113 torchvision==0.11.2+cu113 torchaudio==0.10.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
    "!pip3 install transformers\n",
    "!pip3 install sentencepiece\n",
    "!pip3 install wandb --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import PegasusModel, PegasusForConditionalGeneration, PegasusTokenizerFast, get_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(hash(\"setting random seeds\") % 2**32 - 1)\n",
    "np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n",
    "torch.manual_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n",
    "torch.cuda.manual_seed_all(hash(\"so runs are repeatable\") % 2**32 - 1)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: yenting-thesis (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Experiment and Pipeline\n",
    "##### Track metadata and hyperparameters with wandb.init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    epochs = 10,\n",
    "    batch_size = 16,\n",
    "    optimizer = \"AdamW\",\n",
    "    learning_rate = 5e-5,\n",
    "    weight_decay = 0.01,\n",
    "    loss_function = \"triplet-margin-loss\",\n",
    "    dataset = \"BPMAI-29-10-2019\",\n",
    "    architecture = \"encoder-seq2seq-Pegasus\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline(hyperparameters):\n",
    "\n",
    "    # tell wandb to get started\n",
    "    with wandb.init(project=\"thesis\", entity=\"yenting-thesis\", config=hyperparameters):\n",
    "        # access all HPs through wandb.config, so logging matches execution!\n",
    "        config = wandb.config\n",
    "\n",
    "        # make the model, data, and optimization problem\n",
    "#         model, train_loader, test_loader, optimizer = make(config)\n",
    "        model, train_loader, optimizer = make(config)\n",
    "#         print(model)\n",
    "\n",
    "        # and use them to train the model\n",
    "        train(model, train_loader, optimizer, config)\n",
    "\n",
    "        # and test its final performance\n",
    "#         test(model, test_loader)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make(config):\n",
    "    # Make the pretrained tokenizer and model\n",
    "    model_name = 'google/pegasus-large' # 'google/pegasus-xsum'\n",
    "    tokenizer = PegasusTokenizerFast.from_pretrained(model_name)\n",
    "    model = PegasusForConditionalGeneration.from_pretrained(model_name, return_dict=True)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Make the data\n",
    "    train_loader = make_loader(graph_data, tokenizer, shuffle=True, batch_size=config.batch_size)\n",
    "#     test_loader = make_loader(test, batch_size=config.batch_size)\n",
    "    for anchor, positive, negative in train_loader:\n",
    "        break\n",
    "    print({k: v.shape for k, v in anchor.items()})\n",
    "\n",
    "    # Make the model\n",
    "    kepler_pegasus_model = KeplerPegasusModel(model)\n",
    "    \n",
    "    # Make optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), weight_decay=config.weight_decay, lr=config.learning_rate)\n",
    "    \n",
    "#     return model, train_loader, test_loader, optimizer\n",
    "    return kepler_pegasus_model, train_loader, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Data Loading and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./triplet_train_dataset.json', 'r') as f:\n",
    "    graph_data = json.load(f)\n",
    "    \n",
    "graph_data = graph_data['negatives'] + graph_data['hard_negatives']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, triplets, tokenizer):\n",
    "        self.triplets = triplets\n",
    "        self.tokenizer = tokenizer\n",
    "    def __getitem__(self, idx):\n",
    "        triplet = self.triplets[idx]\n",
    "        triplet_encodings = self.tokenizer(triplet, truncation=True, padding='max_length', max_length=50)\n",
    "        anchor = {key: torch.tensor(val[0]) for key, val in triplet_encodings.items()}\n",
    "        positive = {key: torch.tensor(val[1]) for key, val in triplet_encodings.items()}\n",
    "        negative = {key: torch.tensor(val[2]) for key, val in triplet_encodings.items()}\n",
    "        anchor['labels'] = torch.tensor(triplet_encodings['input_ids'][0])\n",
    "        positive['labels'] = torch.tensor(triplet_encodings['input_ids'][1])\n",
    "        negative['labels'] = torch.tensor(triplet_encodings['input_ids'][2])\n",
    "        return anchor, positive, negative\n",
    "    def __len__(self):\n",
    "        return len(self.triplets) # len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loader(dataset, tokenizer, shuffle, batch_size):\n",
    "    graph_dataset = GraphDataset(dataset, tokenizer)\n",
    "    graphdata_train_dataloader = DataLoader(\n",
    "        dataset=graph_dataset, shuffle=shuffle, batch_size=batch_size\n",
    "    )    \n",
    "\n",
    "    return graphdata_train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eos_idx(batch):\n",
    "    for input_ids in batch['input_ids']:\n",
    "        eos_id = input_ids == 1\n",
    "        idx = eos_id.nonzero()[0]\n",
    "        if 'eos_idx' in locals():\n",
    "            eos_idx = torch.cat((eos_idx, idx), 0)\n",
    "        else:\n",
    "            eos_idx = eos_id.nonzero()[0]\n",
    "    return eos_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeplerPegasusModel(nn.TripletMarginLoss):\n",
    "    \n",
    "    def __init__(self, model, margin: float = 1.0, p: float = 2., eps: float = 1e-6, \n",
    "                 swap: bool = False, size_average=None, reduce=None, reduction: str = 'mean'):\n",
    "        super().__init__(margin, p, eps, swap, size_average, reduce, reduction)\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self, graphdata_batch):\n",
    "\n",
    "        # triplet margin loss\n",
    "        anchors, positives, negatives = graphdata_batch[0], graphdata_batch[1], graphdata_batch[2]\n",
    "        # get eos indices\n",
    "        anchor_eos, positive_eos, negative_eos = get_eos_idx(anchors), get_eos_idx(positives), get_eos_idx(negatives)\n",
    "        # get model encoder output\n",
    "        model_output_a = self.model(**anchors)\n",
    "        model_output_p = self.model(**positives)\n",
    "        model_output_n = self.model(**negatives)\n",
    "        encoder_output_a = model_output_a.encoder_last_hidden_state\n",
    "        encoder_output_p = model_output_p.encoder_last_hidden_state\n",
    "        encoder_output_n = model_output_n.encoder_last_hidden_state\n",
    "        # extract eos embeddings\n",
    "        a_eos = torch.vstack([encoder_output_a[i][anchor_eos[i]] for i in range(encoder_output_a.size(0))])\n",
    "        p_eos = torch.vstack([encoder_output_p[i][positive_eos[i]] for i in range(encoder_output_p.size(0))])\n",
    "        n_eos = torch.vstack([encoder_output_n[i][negative_eos[i]] for i in range(encoder_output_n.size(0))])     \n",
    "        # compute the loss\n",
    "        triplet_margin_loss = F.triplet_margin_loss(a_eos, p_eos, n_eos, \n",
    "                                                    margin=self.margin, p=self.p,\n",
    "                                                    eps=self.eps, swap=self.swap, \n",
    "                                                    reduction=self.reduction)        \n",
    "        \n",
    "        return triplet_margin_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Training Logic\n",
    "##### Track gradients with wandb.watch and everything else with wandb.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, config):\n",
    "    # Tell wandb to watch what the model gets up to: gradients, weights, and more!\n",
    "    wandb.watch(model, log=\"all\", log_freq=10)\n",
    "\n",
    "    # Run training and track with wandb\n",
    "    total_batches = len(loader) * config.epochs\n",
    "    print('num_training_steps', total_batches)\n",
    "    progress_bar = tqdm(range(total_batches))\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=500,\n",
    "        num_training_steps=total_batches,\n",
    "    )\n",
    "    batch_ct = 0\n",
    "    for epoch in range(config.epochs):\n",
    "        for _, graphdata_batch in enumerate(loader):                                 \n",
    "            loss = train_batch(graphdata_batch, model, optimizer, lr_scheduler, progress_bar)\n",
    "            batch_ct += 1\n",
    "            \n",
    "            # Report metrics every 25th batch\n",
    "            if ((batch_ct) % 25) == 0:\n",
    "                train_log(loss, batch_ct, epoch)\n",
    "\n",
    "\n",
    "def train_batch(batch, model, optimizer, lr_scheduler, progress_bar):\n",
    "                                                                                    \n",
    "    graphdata_batch = []\n",
    "    for item in batch:\n",
    "        graphdata_batch.append({k: v.to(device) for k, v in item.items()})\n",
    "                                            \n",
    "    # Forward pass ➡\n",
    "    loss = model(graphdata_batch)                                        \n",
    "    \n",
    "    # Backward pass ⬅\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Step with optimizer and lr_scheduler\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "    progress_bar.update(1)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_log(loss, batch_num, epoch):\n",
    "    # Where the magic happens\n",
    "    wandb.log({\"epoch\": epoch, \"loss\": loss}, step=batch_num)\n",
    "    print(f\"Loss after \" + str(batch_num).zfill(5) + f\" steps: {loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/wandb/run-20220310_222728-lax7zl2f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/yenting-thesis/thesis/runs/lax7zl2f\" target=\"_blank\">ethereal-durian-34</a></strong> to <a href=\"https://wandb.ai/yenting-thesis/thesis\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': torch.Size([16, 50]), 'attention_mask': torch.Size([16, 50]), 'labels': torch.Size([16, 50])}\n",
      "num_training_steps 1230\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2383ffc2145b4a0ebafe7473e6253cdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1230.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 00025 steps: 0.874\n",
      "Loss after 00050 steps: 0.886\n",
      "Loss after 00075 steps: 0.809\n",
      "Loss after 00100 steps: 0.979\n",
      "Loss after 00125 steps: 0.958\n",
      "Loss after 00150 steps: 0.863\n",
      "Loss after 00175 steps: 0.919\n",
      "Loss after 00200 steps: 0.966\n",
      "Loss after 00225 steps: 0.611\n",
      "Loss after 00250 steps: 0.954\n",
      "Loss after 00275 steps: 0.785\n",
      "Loss after 00300 steps: 0.955\n",
      "Loss after 00325 steps: 0.897\n",
      "Loss after 00350 steps: 0.923\n",
      "Loss after 00375 steps: 0.711\n",
      "Loss after 00400 steps: 0.635\n",
      "Loss after 00425 steps: 0.768\n",
      "Loss after 00450 steps: 0.782\n",
      "Loss after 00475 steps: 0.716\n",
      "Loss after 00500 steps: 0.759\n",
      "Loss after 00525 steps: 1.182\n",
      "Loss after 00550 steps: 0.841\n",
      "Loss after 00575 steps: 0.681\n",
      "Loss after 00600 steps: 0.820\n",
      "Loss after 00625 steps: 0.276\n",
      "Loss after 00650 steps: 0.693\n",
      "Loss after 00675 steps: 0.162\n",
      "Loss after 00700 steps: 0.595\n"
     ]
    }
   ],
   "source": [
    "# Build, train and analyze the model with the pipeline\n",
    "model = model_pipeline(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

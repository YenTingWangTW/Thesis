{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-jI2xeXjeAds"
   },
   "source": [
    "### Pegasus-Finetuned and Automatic Evaluation\n",
    "#### Fine-tune pretrained Pegasus using labeled process data\n",
    "- Experiment:\n",
    "    - Summarization as a downstream task to generate process labels \n",
    "    - Model is fine-tuned with labeled process data\n",
    "    - Generated labels is automatically evaluated with metrics BERTScore\n",
    "- Process data:\n",
    "    - Document (process text) and summary (process label)\n",
    "- Outline:\n",
    "    - Track the experiment and its results with WandB (Weights & Biases)\n",
    "    - Define the experiment, data loading, training and validation \n",
    "    - Validation using BERTScore after each training epoch to decide the final output model\n",
    "    - Automatic evaluation using BERTScore\n",
    "\n",
    "\n",
    "#### Reference\n",
    "- Pegasus Hugging Face: \n",
    "https://huggingface.co/docs/transformers/model_doc/pegasus\n",
    "- Hugging Face Transformer fine-tuning tutorial:\n",
    "https://huggingface.co/docs/transformers/training\n",
    "- BERTScore github: \n",
    "https://github.com/Tiiiger/bert_score\n",
    "- WandB pipeline:\n",
    "https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Simple_PyTorch_Integration.ipynb#scrollTo=FH61NWlVR_SL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IN3OUyEzeAeW"
   },
   "source": [
    "#### Environment Setup \n",
    "- Amazon SageMaker Studio\n",
    "- Kernel - Python 3 (Data Science)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NUTiCLeGeAeX"
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip3 install torch==1.10.1+cu113 torchvision==0.11.2+cu113 torchaudio==0.10.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
    "# !pip3 install transformers\n",
    "# !pip3 install sentencepiece\n",
    "# !pip3 install wandb --upgrade\n",
    "# !pip3 install bert-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uyJnLHBXeAeZ"
   },
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zxphzH0HeAea"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import PegasusConfig, PegasusModel, PegasusForConditionalGeneration, PegasusTokenizerFast, get_scheduler\n",
    "from transformers.optimization import Adafactor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bert_score import BERTScorer\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHklLwybeAea"
   },
   "source": [
    "#### WandB Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Gv2uXGvweAeb"
   },
   "outputs": [],
   "source": [
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(hash(\"setting random seeds\") % 2**32 - 1)\n",
    "np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n",
    "torch.manual_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n",
    "torch.cuda.manual_seed_all(hash(\"so runs are repeatable\") % 2**32 - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "o_lNCxGzeAec"
   },
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vy3IqeFteAed",
    "outputId": "cdfc7b8c-865a-491d-c653-3aa9ca04a5c7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myentingwang\u001b[0m (\u001b[33myenting-thesis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7F7A1fcYeAee"
   },
   "source": [
    "#### Define the Experiment and Pipeline \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "EL14SAGneAef"
   },
   "outputs": [],
   "source": [
    "# Define the configuration of the experiment\n",
    "config = dict(\n",
    "    epochs = 15,\n",
    "    batch_size = 20, # or 22\n",
    "    optimizer = \"adafactor\",\n",
    "    loss_function = \"summarization-loss\", # loss calculated given ground truth summaries (process names)\n",
    "    dataset = \"bpmai-29-10-2019\",\n",
    "    architecture = \"seq2seq-pegasus\",\n",
    "    retrain = False, # True if continue training from checkpoint of previous iteration\n",
    "    input_model = \"\", # specify path of input model if continue training or left blank\n",
    "    output_model = \"./model_summarization/summarization_{}_epoch.pth\" # specify path to save output model, i.e. \"./model_summarization/summarization_{}_epoch.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LdXDhOV2eAeg"
   },
   "source": [
    "##### Track metadata and hyperparameters with wandb.init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "c_rHusIieAeh"
   },
   "outputs": [],
   "source": [
    "# Define the training pipeline\n",
    "def model_pipeline(hyperparameters):\n",
    "    with wandb.init(project=\"wandb-project-name\", entity=\"yenting-thesis\", config=hyperparameters):\n",
    "        config = wandb.config\n",
    "        # set model, data loader, tokenizer and optimizer with defined config\n",
    "        model, train_loader, tokenizer, optimizer = make(config)\n",
    "        # train and validate\n",
    "        train_and_val(model, train_loader, doc_val, sum_val, tokenizer, optimizer, config)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euOYCOpieAei"
   },
   "source": [
    "##### Set model, data loaders and optimizer with defined configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "O_j8y_sDeAej"
   },
   "outputs": [],
   "source": [
    "def make(config):\n",
    "    # set pretrained tokenizer, model and optimizer\n",
    "    model_name = 'google/pegasus-large' # 'google/pegasus-xsum'\n",
    "    tokenizer = PegasusTokenizerFast.from_pretrained(model_name)\n",
    "    model = PegasusForConditionalGeneration.from_pretrained(model_name, return_dict=True)\n",
    "    optimizer = Adafactor(model.parameters(), scale_parameter=True, relative_step=True, warmup_init=True, lr=None)\n",
    "    \n",
    "    # if continue training from checkpoint of previous iteration\n",
    "    if config.retrain: \n",
    "        load(model, optimizer, config.input_model)\n",
    "        model = PegasusForConditionalGeneration.from_pretrained(model_name, output_hidden_states=True, output_attentions=True, return_dict=True)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "            model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "    \n",
    "    # set data loaders\n",
    "    train_loader = make_loader(doc_train, sum_train, tokenizer, shuffle=True, batch_size=config.batch_size)\n",
    "    # test print data\n",
    "    for batch in train_loader:\n",
    "        break\n",
    "    print({k: v.shape for k, v in batch.items()})\n",
    "    \n",
    "    return model, train_loader, tokenizer, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4UnCKT9eAej"
   },
   "source": [
    "#### Define Data Loading\n",
    "#### Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "bSiOrIg3eAek"
   },
   "outputs": [],
   "source": [
    "with open('./data/train_test_labeled_dataset.json', 'r') as f:\n",
    "    process = json.load(f)\n",
    "doc_train, doc_val, sum_train, sum_val = train_test_split(process['document_train'], process['summary_train'], test_size=0.175, random_state=41)\n",
    "doc_test = process['document_test']\n",
    "sum_test = process['summary_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Is8huvgjeAek"
   },
   "source": [
    "##### Load augmented train, val and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "CS2VojK-eAel"
   },
   "outputs": [],
   "source": [
    "# with open('./data/aug_train_val_test_labeled_dataset.json', 'r') as f:\n",
    "#     process = json.load(f)\n",
    "# doc_train = process['document_train']\n",
    "# sum_train = process['summary_train']\n",
    "# doc_val = process['document_val']\n",
    "# sum_val = process['summary_val']\n",
    "# doc_test = process['document_test']\n",
    "# sum_test = process['summary_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BgMOZzjteAel"
   },
   "source": [
    "##### Define Process Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "JHc_V7OTeAem"
   },
   "outputs": [],
   "source": [
    "class ProcessDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels['input_ids'][idx]) # torch.tensor(self.labels[idx])\n",
    "        return item # input_ids, attention_mask, labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels['input_ids']) # len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ly2a9oW9eAem"
   },
   "source": [
    "##### Define Process Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "x6Qz9PToeAem"
   },
   "outputs": [],
   "source": [
    "def make_loader(texts, labels, tokenizer, shuffle, batch_size):\n",
    "    process_dataset = process_data(texts, labels, tokenizer)\n",
    "    process_dataloader = DataLoader(\n",
    "        process_dataset, shuffle=shuffle, batch_size=batch_size\n",
    "    )  \n",
    "    return process_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "QaVz4krNeAep"
   },
   "outputs": [],
   "source": [
    "# Define function needed to tokenize texts and labels\n",
    "def process_data(texts, labels, tokenizer):\n",
    "    encodings = tokenizer(texts, truncation=True, padding=True)\n",
    "    decodings = tokenizer(labels, truncation=True, padding=True)\n",
    "    process_dataset = ProcessDataset(encodings, decodings)\n",
    "    return process_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAgQRGCWeAeq"
   },
   "source": [
    "#### Define Training Logic\n",
    "##### Track gradients and weights with wandb.watch and everything else, i.e. loss, with wandb.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "4sD0wwO_eAeq"
   },
   "outputs": [],
   "source": [
    "def train_and_val(model, train_loader, doc_val, sum_val, tokenizer, optimizer, config):\n",
    "    # set the model to train\n",
    "    wandb.watch(model, log=\"all\", log_freq=10)\n",
    "\n",
    "    # run training and track with wandb\n",
    "    total_batches = len(train_loader) * config.epochs\n",
    "    print('num_training_steps', total_batches)\n",
    "    progress_bar = tqdm(range(total_batches))\n",
    "\n",
    "    batch_ct = 0\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    model_save_epoch = 0\n",
    "    for epoch in range(config.epochs):\n",
    "        model.train()\n",
    "        for idx, process_batch in enumerate(train_loader):\n",
    "            loss = train_batch(idx, process_batch, model, optimizer, progress_bar)\n",
    "            batch_ct += 1\n",
    "            # report metrics every 5th batch\n",
    "            running_loss += loss.item()\n",
    "            if (batch_ct % 5) == 0:\n",
    "                last_loss = running_loss / 5 # log loss in average term\n",
    "                train_log(last_loss, batch_ct, epoch)\n",
    "                running_loss = 0.\n",
    "        # validate model after train at each epoch\n",
    "        model.eval()\n",
    "        P, R, F1 = val(model, tokenizer, doc_val, sum_val)\n",
    "        val_log(P, R, F1) # log validation loss        \n",
    "            \n",
    "        # save model after train each epoch\n",
    "        if epoch >= model_save_epoch:\n",
    "            output_model = config.output_model.format(epoch+1)\n",
    "            save(model, optimizer, output_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kq6N6j-QeAer"
   },
   "source": [
    "##### Define functions needed in the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "xjxcYpareAer"
   },
   "outputs": [],
   "source": [
    "def train_batch(idx, batch, model, optimizer, progress_bar):                                                                                 \n",
    "    process_item = {k: v.to(device) for k, v in batch.items()}        \n",
    "    # forward pass\n",
    "    model_output = model(**process_item)\n",
    "    loss = model_output.loss    \n",
    "    # backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # step with optimizer\n",
    "    optimizer.step()\n",
    "    progress_bar.update(1)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "97uhJpvseAer"
   },
   "outputs": [],
   "source": [
    "def val(model, tokenizer, doc_val, sum_val):\n",
    "    P = torch.tensor([])\n",
    "    R = torch.tensor([])\n",
    "    F1 = torch.tensor([])\n",
    "    doc_list = np.array(doc_val).reshape((9, 7)) # should adapt reshape size according to different input\n",
    "    sum_list = np.array(sum_val).reshape((9, 7))\n",
    "    scorer = BERTScorer(lang=\"en\", rescale_with_baseline=True)\n",
    "    with torch.no_grad():\n",
    "        for document, summary in zip(doc_list, sum_list):\n",
    "            doc_tokenized = tokenizer(document.tolist(), truncation=True, padding='longest', return_tensors=\"pt\").to(device)\n",
    "            translated = model.generate(**doc_tokenized)\n",
    "            generated = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "            P_temp, R_temp, F1_temp = scorer.score(generated, summary.tolist())\n",
    "            P = torch.cat([P, P_temp])\n",
    "            R = torch.cat([R, R_temp])\n",
    "            F1 = torch.cat([F1, F1_temp])\n",
    "        P = P.mean()\n",
    "        R = R.mean()\n",
    "        F1 = F1.mean()\n",
    "    return P, R, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "mJFP_UKXeAes"
   },
   "outputs": [],
   "source": [
    "def train_log(loss, batch_num, epoch):\n",
    "    wandb.log({\"epoch\": epoch, \"loss\": loss}, step=batch_num)\n",
    "    print(f\"Loss after \" + str(batch_num).zfill(5) + f\" steps: {loss:.3f}\")\n",
    "    \n",
    "def val_log(P, R, F1):\n",
    "    wandb.log({\"bert_score_P\": P, \"bert_score_R\": R, \"bert_score_F1\": F1})\n",
    "    print(f\"bert_score_P: {P:.3f}, bert_score_R, {R:.3f}, bert_score_F1, {F1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "DuutG9FceAes"
   },
   "outputs": [],
   "source": [
    "def save(model, optimizer, output_model):\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, output_model)\n",
    "\n",
    "def load(model, optimizer, output_model):\n",
    "    checkpoint = torch.load(output_model)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KlJWmp3DeAet"
   },
   "source": [
    "#### Build, train and analyze the model with the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lt8E0Y0qeAet"
   },
   "outputs": [],
   "source": [
    "# Build, train and analyze the model with the pipeline\n",
    "model = model_pipeline(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPVjpBE2eAet"
   },
   "source": [
    "#### Automatic Evaluation on Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RE5gGycbeAet"
   },
   "source": [
    "##### Define functions needed in the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "2i3ItkebeAet"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, tokenizer, doc_test, sum_test):\n",
    "    generated_summary = []\n",
    "    P = torch.tensor([])\n",
    "    R = torch.tensor([])\n",
    "    F1 = torch.tensor([])\n",
    "    doc_list = np.array(doc_test).reshape((10, 9)) # should adapt reshape size according to different input\n",
    "    sum_list = np.array(sum_test).reshape((10, 9))\n",
    "    scorer = BERTScorer(lang=\"en\", rescale_with_baseline=True)\n",
    "    with torch.no_grad():\n",
    "        for document, summary in zip(doc_list, sum_list):\n",
    "            doc_tokenized = tokenizer(document.tolist(), truncation=True, padding='longest', return_tensors=\"pt\").to(device)\n",
    "            translated = model.generate(**doc_tokenized)\n",
    "            generated = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "            P_temp, R_temp, F1_temp = scorer.score(generated, summary.tolist())\n",
    "            P = torch.cat([P, P_temp])\n",
    "            R = torch.cat([R, R_temp])\n",
    "            F1 = torch.cat([F1, F1_temp])\n",
    "            generated_summary += generated     \n",
    "        P = P.mean()\n",
    "        R = R.mean()\n",
    "        F1 = F1.mean()\n",
    "        \n",
    "    return P, R, F1, generated_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "PqWnBRCmeAeu"
   },
   "outputs": [],
   "source": [
    "def eval_log(P, R, F1):\n",
    "    wandb.log({\"bert_score_P_eval\": P, \"bert_score_R_eval\": R, \"bert_score_F1_eval\": F1})\n",
    "    print(f\"bert_score_P: {P:.3f}, bert_score_R, {R:.3f}, bert_score_F1, {F1:.3f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-3P2UNkeAeu"
   },
   "source": [
    "##### Track metadata with wandb.init and run evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "dWJGl4deeAeu"
   },
   "outputs": [],
   "source": [
    "# Define the configuration of the evaluation\n",
    "config = dict(\n",
    "    input_model = \"\", # specify path of the trained input model, i.e. \"./model_summarization/summarization_7_epoch.pth\"\n",
    "    file_name = \"\" # name the output file of the produced labels, i.e. \"generated_labels.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KMdCvroMeAev"
   },
   "outputs": [],
   "source": [
    "with wandb.init(project=\"wandb-project-name\", entity=\"yenting-thesis\"): \n",
    "    model_name = 'google/pegasus-large'\n",
    "    model = PegasusForConditionalGeneration.from_pretrained(model_name, return_dict=True).to(device)\n",
    "    tokenizer = PegasusTokenizerFast.from_pretrained(model_name)\n",
    "    optimizer = Adafactor(model.parameters(), scale_parameter=True, relative_step=True, warmup_init=True, lr=None)\n",
    "    wandb.watch(model, log=\"all\", log_freq=10)\n",
    "    # load input model\n",
    "    input_model = config['input_model']\n",
    "    load(model, optimizer, input_model)\n",
    "    # model eval\n",
    "    model.eval()\n",
    "    P, R, F1, generated_summary = evaluate(model, tokenizer, doc_test, sum_test)\n",
    "    # output generated labels or summary in the file\n",
    "    with open(config['file_name'], 'w') as f:\n",
    "        for line in generated_summary:\n",
    "            f.write(line)\n",
    "            f.write('\\n')\n",
    "    eval_log(P, R, F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUkBD8sWeAev"
   },
   "source": [
    "#### Inspect the Generated Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z-ERZc8DeAev"
   },
   "outputs": [],
   "source": [
    "for i, (ref, hypo) in enumerate(zip(sum_test, generated_summary)):\n",
    "    print(\"ref: \" + ref + \"\\n\" + \"hypo: \" + hypo + \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9TaPoQ8srudZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

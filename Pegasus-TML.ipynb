{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pegasus-TML\n",
    "\n",
    "#### Additional pre-training task on encoder Pegasus to gain knowledge of Control-flow relations in processes\n",
    "- Experiment:\n",
    "    - Pegasus is given another pre-training task, Control-flow Relation Learning, using Triplet Margin Loss (TML) training \n",
    "    - Based on the control-flow relations exist in the processes, triplets (anchor, pos, neg) are extracted from the process text \n",
    "- Process data:\n",
    "    - Triplets (anchor, pos, neg)\n",
    "- Outline:\n",
    "    - Track the experiment and its results with WandB (Weights & Biases)\n",
    "    - Define the experiment, data loading, training and validation \n",
    "    - Validation loss is tracked to apply early stopping and prevent overfitting\n",
    "\n",
    "#### Reference\n",
    "- Pegasus Hugging Face: \n",
    "https://huggingface.co/docs/transformers/model_doc/pegasus\n",
    "- Hugging Face Fine-tuning Transformer tutorial:\n",
    "https://huggingface.co/docs/transformers/training\n",
    "- TripletMarginLoss:\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.TripletMarginLoss.html\n",
    "- WandB pipeline:\n",
    "https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Simple_PyTorch_Integration.ipynb#scrollTo=FH61NWlVR_SL\n",
    "- Early stopping:\n",
    "https://wandb.ai/ayush-thakur/huggingface/reports/Early-Stopping-in-HuggingFace-Examples--Vmlldzo0MzE2MTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment Setup \n",
    "- Amazon SageMaker Studio\n",
    "- Kernel - Python 3 (Data Science)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip3 install torch==1.10.1+cu113 torchvision==0.11.2+cu113 torchaudio==0.10.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
    "# !pip3 install transformers\n",
    "# !pip3 install sentencepiece\n",
    "# !pip3 install wandb --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizerFast\n",
    "from transformers.optimization import Adafactor\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WandB Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(hash(\"setting random seeds\") % 2**32 - 1)\n",
    "np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n",
    "torch.manual_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n",
    "torch.cuda.manual_seed_all(hash(\"so runs are repeatable\") % 2**32 - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Experiment and Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the configuration of the experiment\n",
    "config = dict(\n",
    "    epochs = 20,\n",
    "    batch_size = 16,\n",
    "    optimizer = \"adafactor\",\n",
    "    es_patience = 5, # early stopping patience steps\n",
    "    loss_function = \"triplet-margin-loss\",\n",
    "    dataset = \"bpmai-29-10-2019\",\n",
    "    architecture = \"encoder-seq2seq-pegasus\", # TML trained on the encoder part of Pegasus model\n",
    "    retrain = False, # True if continue training from checkpoint of previous iteration\n",
    "    input_model = \"./models_TML/TML_n_epoch.pth\" # specify path of input model if continue training or left blank\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Track metadata and hyperparameters with wandb.init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training pipeline\n",
    "def model_pipeline(hyperparameters):\n",
    "    with wandb.init(project=\"wandb-project-name\", entity=\"wandb-entity-name\", config=hyperparameters):\n",
    "        config = wandb.config\n",
    "        # set model, data loaders, optimizer, and early stopping with defined config\n",
    "        model, train_loader, val_loader, optimizer = make(config)\n",
    "        es = EarlyStopping(patience = config.es_patience)\n",
    "        # train and validate with early stopping applied\n",
    "        train_and_val(model, train_loader, val_loader, optimizer, es, config)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set model, data loaders and optimizer with defined configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make(config):\n",
    "    # set pretrained tokenizer, model and optimizer\n",
    "    model_name = 'google/pegasus-large' # 'google/pegasus-xsum'\n",
    "    tokenizer = PegasusTokenizerFast.from_pretrained(model_name)\n",
    "    model = PegasusForConditionalGeneration.from_pretrained(model_name, return_dict=True)\n",
    "    optimizer = Adafactor(model.parameters(), scale_parameter=True, relative_step=True, warmup_init=True, lr=None)\n",
    "    \n",
    "    # if continue training from checkpoint of previous iteration\n",
    "    if config.retrain: \n",
    "        load(model, optimizer, config.input_model)\n",
    "        model = PegasusForConditionalGeneration.from_pretrained(model_name, output_hidden_states=True, output_attentions=True, return_dict=True)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "            model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "    \n",
    "    # set data loaders\n",
    "    train_loader = make_loader(train_data, tokenizer, shuffle=True, batch_size=config.batch_size)\n",
    "    val_loader = make_loader(val_data, tokenizer, shuffle=True, batch_size=config.batch_size)\n",
    "    # test print data\n",
    "    for anchor, positive, negative in train_loader:\n",
    "        break\n",
    "    print({k: v.shape for k, v in anchor.items()})\n",
    "    \n",
    "    return model, train_loader, val_loader, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Data Loading and Model\n",
    "##### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/triplet_train_dataset.json', 'r') as f:\n",
    "    t_data = json.load(f)\n",
    "with open('./data/triplet_val_dataset.json', 'r') as f:\n",
    "    v_data = json.load(f)\n",
    "\n",
    "train_data = []\n",
    "val_data = []\n",
    "train_temp = t_data['easy_negatives'] + t_data['negatives'] + t_data['one_step_away_negs'] + t_data['hard_negatives']\n",
    "val_temp = v_data['easy_negatives'] + v_data['negatives'] + v_data['one_step_away_negs'] + v_data['hard_negatives']\n",
    "for d1 in train_temp:\n",
    "    train_data.append([x.lower() for x in d1])\n",
    "for d2 in val_temp:\n",
    "    val_data.append([x.lower() for x in d2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define Triplet Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, triplets, tokenizer):\n",
    "        self.triplets = triplets\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        triplet = self.triplets[idx]\n",
    "        triplet_encodings = self.tokenizer(triplet, truncation=True, padding='max_length', max_length=50)\n",
    "        anchor = {key: torch.tensor(val[0]) for key, val in triplet_encodings.items()}\n",
    "        positive = {key: torch.tensor(val[1]) for key, val in triplet_encodings.items()}\n",
    "        negative = {key: torch.tensor(val[2]) for key, val in triplet_encodings.items()}\n",
    "        # set dummy labels for the decoder part of Pegasus, which is left untrained in this model training process\n",
    "        anchor['labels'] = torch.tensor(triplet_encodings['input_ids'][0])\n",
    "        positive['labels'] = torch.tensor(triplet_encodings['input_ids'][1])\n",
    "        negative['labels'] = torch.tensor(triplet_encodings['input_ids'][2])\n",
    "        return anchor, positive, negative # input_ids, attention_mask, labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.triplets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define Triplet Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loader(triplets, tokenizer, shuffle, batch_size):\n",
    "    triplet_dataset = TripletDataset(triplets, tokenizer)\n",
    "    triplet_dataloader = DataLoader(\n",
    "        dataset=triplet_dataset, shuffle=shuffle, batch_size=batch_size\n",
    "    )\n",
    "    return triplet_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define Pegasus-TML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PegasusTMLModel(nn.TripletMarginLoss):\n",
    "    def __init__(self, model, margin: float = 1.0, p: float = 2., eps: float = 1e-6, \n",
    "                 swap: bool = False, size_average=None, reduce=None, reduction: str = 'mean'):\n",
    "        super().__init__(margin, p, eps, swap, size_average, reduce, reduction)\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self, triplet_batch):       \n",
    "        # retrieve triplets in batch\n",
    "        anchors, positives, negatives = triplet_batch[0], triplet_batch[1], triplet_batch[2]\n",
    "        # get index of </s> end-of-sentence token representing each anchor, pos and neg\n",
    "        anchor_eos, positive_eos, negative_eos = get_eos_idx(anchors), get_eos_idx(positives), get_eos_idx(negatives)\n",
    "        # get model outputs of anchor, pos and neg sentences\n",
    "        model_output_a = self.model(**anchors)\n",
    "        model_output_p = self.model(**positives)\n",
    "        model_output_n = self.model(**negatives)\n",
    "        # get </s> token from encoder output - last hidden layer\n",
    "        encoder_output_a = model_output_a.encoder_last_hidden_state\n",
    "        encoder_output_p = model_output_p.encoder_last_hidden_state\n",
    "        encoder_output_n = model_output_n.encoder_last_hidden_state\n",
    "        a_eos = torch.vstack([encoder_output_a[i][anchor_eos[i]] for i in range(encoder_output_a.size(0))])\n",
    "        p_eos = torch.vstack([encoder_output_p[i][positive_eos[i]] for i in range(encoder_output_p.size(0))])\n",
    "        n_eos = torch.vstack([encoder_output_n[i][negative_eos[i]] for i in range(encoder_output_n.size(0))])     \n",
    "        # compute the loss\n",
    "        triplet_margin_loss = F.triplet_margin_loss(a_eos, p_eos, n_eos, \n",
    "                                                    margin=self.margin, p=self.p,\n",
    "                                                    eps=self.eps, swap=self.swap, \n",
    "                                                    reduction=self.reduction)        \n",
    "\n",
    "        return triplet_margin_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function used to extract the indexes of </s> end-of-sentence tokens representing each sentence\n",
    "def get_eos_idx(batch):\n",
    "    for input_ids in batch['input_ids']:\n",
    "        eos_id = input_ids == 1\n",
    "        idx = eos_id.nonzero()[0]\n",
    "        if 'eos_idx' in locals():\n",
    "            eos_idx = torch.cat((eos_idx, idx), 0)\n",
    "        else:\n",
    "            eos_idx = eos_id.nonzero()[0]\n",
    "    return eos_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Early Stopping \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping(object):\n",
    "    def __init__(self, mode='min', min_delta=0, patience=10, percentage=False):\n",
    "        self.mode = mode\n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "        self.best = None\n",
    "        self.num_bad_epochs = 0\n",
    "        self.is_better = None\n",
    "        self._init_is_better(mode, min_delta, percentage)\n",
    "\n",
    "        if patience == 0:\n",
    "            self.is_better = lambda a, b: True\n",
    "            self.step = lambda a: False\n",
    "\n",
    "    def step(self, metrics):\n",
    "        if self.best is None:\n",
    "            self.best = metrics\n",
    "            return False\n",
    "\n",
    "        if torch.isnan(metrics):\n",
    "            return True\n",
    "\n",
    "        if self.is_better(metrics, self.best):\n",
    "            self.num_bad_epochs = 0\n",
    "            self.best = metrics\n",
    "        else:\n",
    "            self.num_bad_epochs += 1\n",
    "\n",
    "        if self.num_bad_epochs >= self.patience:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _init_is_better(self, mode, min_delta, percentage):\n",
    "        if mode not in {'min', 'max'}:\n",
    "            raise ValueError('mode ' + mode + ' is unknown!')\n",
    "        if not percentage:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best - min_delta\n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best + min_delta\n",
    "        else:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best - (\n",
    "                            best * min_delta / 100)\n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best + (\n",
    "                            best * min_delta / 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Training Logic\n",
    "##### Track gradients and weights with wandb.watch and everything else, i.e. loss, with wandb.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_val(model, train_loader, val_loader, optimizer, es, config):\n",
    "    # set the model to train\n",
    "    pegasus_tml_model = PegasusTMLModel(model)\n",
    "    wandb.watch(pegasus_tml_model, log=\"all\", log_freq=10)\n",
    "\n",
    "    # run training and track with wandb\n",
    "    total_batches = len(train_loader) * config.epochs\n",
    "    print('num_training_steps', total_batches)\n",
    "    progress_bar = tqdm(range(total_batches))\n",
    "\n",
    "    batch_ct = 0\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "#     model_save_epoch = 1\n",
    "    for epoch in range(config.epochs):\n",
    "        model.train()\n",
    "        for idx, triplet_batch in enumerate(train_loader):\n",
    "            loss = train_batch(idx, triplet_batch, pegasus_tml_model, optimizer, progress_bar)\n",
    "            batch_ct += 1\n",
    "            # report metrics every 25th batch\n",
    "            running_loss += loss.item()\n",
    "            if (batch_ct % 25) == 0:\n",
    "                last_loss = running_loss / 25 # log loss in average term\n",
    "                train_log(last_loss, batch_ct, epoch)\n",
    "                running_loss = 0.\n",
    "        # validate model after train at each epoch\n",
    "        model.eval()\n",
    "        val_loss = val(pegasus_tml_model, val_loader, batch_ct, epoch)\n",
    "        val_log(val_loss, batch_num, epoch) # log validation loss\n",
    "        # check whether to apply early stopping (number of patience step)\n",
    "        if es.step(val_loss):\n",
    "            output_model = './models_TML/TML_{}_epoch.pth'.format(epoch+1)\n",
    "            save(model, optimizer, output_model) # save model before training stops\n",
    "            break\n",
    "            \n",
    "#         # save model after train each epoch\n",
    "#         if epoch >= model_save_epoch:\n",
    "#             output_model = './models_TML/TML_{}_epoch.pth'.format(epoch+1)\n",
    "#             save(model, optimizer, output_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define functions needed in the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(idx, batch, model, optimizer, progress_bar):                                                                                 \n",
    "    triplet_items = []\n",
    "    for item in batch:\n",
    "        triplet_items.append({k: v.to(device) for k, v in item.items()}) \n",
    "    # forward pass\n",
    "    loss = model(triplet_items)      \n",
    "    # backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # step with optimizer every 2 step (batch accumulation)\n",
    "    if (idx+1) % 2 == 0:\n",
    "        optimizer.step()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(model, val_loader):\n",
    "    with torch.no_grad():\n",
    "        loss = 0\n",
    "        for _, triplet_batch in enumerate(val_loader):\n",
    "            triplet_items = []\n",
    "            for item in triplet_batch:\n",
    "                triplet_items.append({k: v.to(device) for k, v in item.items()})\n",
    "            loss += model(triplet_items)\n",
    "        # output loss in average\n",
    "        loss /= len(val_loader)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_log(loss, batch_num, epoch):\n",
    "    wandb.log({\"epoch\": epoch, \"loss\": loss}, step=batch_num)\n",
    "    print(f\"Loss after \" + str(batch_num).zfill(5) + f\" steps: {loss:.3f}\")\n",
    "\n",
    "def val_log(loss, batch_num, epoch):\n",
    "    wandb.log({\"val_loss\": loss})\n",
    "    print(f\"Validation Loss after \" + str(batch_num).zfill(5) + f\" training steps: {loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model, optimizer, output_model):\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, output_model)\n",
    "\n",
    "def load(model, optimizer, output_model):\n",
    "    checkpoint = torch.load(output_model)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build, train and analyze the model with the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_pipeline(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install virtualenv\n",
    "# !virtualenv my_env\n",
    "# !python -m ipykernel install --user --name=my_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip3 install torch==1.10.1+cu113 torchvision==0.11.2+cu113 torchaudio==0.10.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
    "!pip3 install transformers\n",
    "!pip3 install sentencepiece\n",
    "!pip3 install wandb --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import PegasusConfig, PegasusModel, PegasusForConditionalGeneration, PegasusTokenizerFast, get_scheduler\n",
    "from transformers.optimization import Adafactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(hash(\"setting random seeds\") % 2**32 - 1)\n",
    "np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n",
    "torch.manual_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n",
    "torch.cuda.manual_seed_all(hash(\"so runs are repeatable\") % 2**32 - 1)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: yenting-thesis (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Experiment and Pipeline\n",
    "##### Track metadata and hyperparameters with wandb.init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    epochs = 20,\n",
    "    batch_size = 2,\n",
    "    optimizer = \"Adafactor\",\n",
    "    learning_rate = 5e-5,\n",
    "    weight_decay = 0.01,\n",
    "    es_patience = 5,\n",
    "    loss_function = \"triplet-margin-loss\",\n",
    "    dataset = \"BPMAI-29-10-2019\",\n",
    "    architecture = \"encoder-seq2seq-Pegasus\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline(hyperparameters):\n",
    "\n",
    "    # tell wandb to get started\n",
    "    with wandb.init(project=\"thesis-TML-maskedSent\", entity=\"yenting-thesis\", config=hyperparameters):\n",
    "        # access all HPs through wandb.config, so logging matches execution!\n",
    "        config = wandb.config\n",
    "\n",
    "        # make the model, data, and optimization problem\n",
    "        model, train_loader, test_loader, optimizer = make(config)\n",
    "#         print(model)\n",
    "\n",
    "        # make the early stopping\n",
    "        es = EarlyStopping(patience = config.es_patience)\n",
    "        \n",
    "        # and use them to train the model\n",
    "        train_and_val(model, train_loader, test_loader, optimizer, es, config)\n",
    "\n",
    "        # and test its final performance\n",
    "#         test(model, test_loader)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make(config):\n",
    "    # Make the pretrained tokenizer and model\n",
    "    model_name = 'google/pegasus-large' # 'google/pegasus-xsum'\n",
    "    tokenizer = PegasusTokenizerFast.from_pretrained(model_name)\n",
    "    model = PegasusForConditionalGeneration.from_pretrained(model_name, return_dict=True)\n",
    "#     if torch.cuda.device_count() > 1:\n",
    "#         print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "#         model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Make the data\n",
    "    train_loader = make_loader(train_data, tokenizer, shuffle=True, batch_size=config.batch_size)\n",
    "    test_loader = make_loader(test_data, tokenizer, shuffle=True, batch_size=config.batch_size)\n",
    "    for batch in train_loader:\n",
    "        break\n",
    "    print({k: v.shape for k, v in batch.items()})\n",
    "#     for anchor, positive, negative in train_loader:\n",
    "#         break\n",
    "#     print({k: v.shape for k, v in anchor.items()})\n",
    "\n",
    "    # Make the model\n",
    "    kepler_pegasus_model = KeplerPegasusModel(model)\n",
    "    \n",
    "    # Make optimizer\n",
    "#     optimizer = torch.optim.AdamW(model.parameters(), weight_decay=config.weight_decay, lr=config.learning_rate)\n",
    "    # replace AdamW with Adafactor\n",
    "    optimizer = Adafactor(model.parameters(), scale_parameter=True, relative_step=True, warmup_init=True, lr=None)\n",
    "#     optimizer = Adafactor(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay, relative_step=False)\n",
    "    \n",
    "    return kepler_pegasus_model, train_loader, test_loader, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Data Loading and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./masked_sent_train.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "with open('./masked_sent_val.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "    \n",
    "# # slice data - can't random pick dataset\n",
    "# train_graph_data = train_data['easy_negatives'] + train_data['negatives'] + train_data['one_step_away_negs'] + train_data['hard_negatives']\n",
    "# test_graph_data = test_data['easy_negatives'] + test_data['negatives'] + test_data['one_step_away_negs'] + test_data['hard_negatives']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubprocessDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels['input_ids'][idx])\n",
    "        return item # input_ids, attention_mask, labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, triplets, tokenizer):\n",
    "        self.triplets = triplets\n",
    "        self.tokenizer = tokenizer\n",
    "    def __getitem__(self, idx):\n",
    "        triplet = self.triplets[idx]\n",
    "        triplet_encodings = self.tokenizer(triplet, truncation=True, padding='max_length', max_length=50)\n",
    "        anchor = {key: torch.tensor(val[0]) for key, val in triplet_encodings.items()}\n",
    "        positive = {key: torch.tensor(val[1]) for key, val in triplet_encodings.items()}\n",
    "        negative = {key: torch.tensor(val[2]) for key, val in triplet_encodings.items()}\n",
    "        anchor['labels'] = torch.tensor(triplet_encodings['input_ids'][0])\n",
    "        positive['labels'] = torch.tensor(triplet_encodings['input_ids'][1])\n",
    "        negative['labels'] = torch.tensor(triplet_encodings['input_ids'][2])\n",
    "        return anchor, positive, negative # input_ids, attention_mask, labels\n",
    "    def __len__(self):\n",
    "        return len(self.triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(texts, labels, tokenizer):\n",
    "    encodings = tokenizer(texts, truncation=True, padding=True)\n",
    "    decodings = tokenizer(labels, truncation=True, padding=True)\n",
    "    return encodings, decodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loader(dataset, tokenizer, shuffle, batch_size):\n",
    "    texts = dataset['document']\n",
    "    labels = dataset['summary']\n",
    "    encodings, decodings = tokenize_data(texts, labels, tokenizer)\n",
    "    subprocess_dataset = SubprocessDataset(encodings, decodings)\n",
    "    subprocess_dataloader = DataLoader(\n",
    "        dataset=subprocess_dataset, shuffle=shuffle, batch_size=batch_size\n",
    "    )\n",
    "#     graph_dataset = GraphDataset(dataset, tokenizer)\n",
    "#     graphdata_dataloader = DataLoader(\n",
    "#         dataset=graph_dataset, shuffle=shuffle, batch_size=batch_size\n",
    "#     )    \n",
    "\n",
    "    return subprocess_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eos_idx(batch):\n",
    "    for input_ids in batch['input_ids']:\n",
    "        eos_id = input_ids == 1\n",
    "        idx = eos_id.nonzero()[0]\n",
    "        if 'eos_idx' in locals():\n",
    "            eos_idx = torch.cat((eos_idx, idx), 0)\n",
    "        else:\n",
    "            eos_idx = eos_id.nonzero()[0]\n",
    "    return eos_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeplerPegasusModel(nn.TripletMarginLoss):\n",
    "    \n",
    "    def __init__(self, model, margin: float = 1.0, p: float = 2., eps: float = 1e-6, \n",
    "                 swap: bool = False, size_average=None, reduce=None, reduction: str = 'mean'):\n",
    "        super().__init__(margin, p, eps, swap, size_average, reduce, reduction)\n",
    "        self.model = model\n",
    "        \n",
    "        # forward(self, graphdata_batch, subprocess_batch)\n",
    "    def forward(self, subprocess_batch):\n",
    "        MLM_output = self.model(**subprocess_batch)\n",
    "        maskedSent_loss = MLM_output.loss\n",
    "        \n",
    "#         anchors, positives, negatives = graphdata_batch[0], graphdata_batch[1], graphdata_batch[2]\n",
    "#         anchor_eos, positive_eos, negative_eos = get_eos_idx(anchors), get_eos_idx(positives), get_eos_idx(negatives)\n",
    "#         # Triplet margin loss\n",
    "#         model_output_a = self.model(**anchors)\n",
    "#         model_output_p = self.model(**positives)\n",
    "#         model_output_n = self.model(**negatives)\n",
    "        \n",
    "#         encoder_output_a = model_output_a.encoder_last_hidden_state\n",
    "#         encoder_output_p = model_output_p.encoder_last_hidden_state\n",
    "#         encoder_output_n = model_output_n.encoder_last_hidden_state\n",
    "        \n",
    "#         a_eos = torch.vstack([encoder_output_a[i][anchor_eos[i]] for i in range(encoder_output_a.size(0))])\n",
    "#         p_eos = torch.vstack([encoder_output_p[i][positive_eos[i]] for i in range(encoder_output_p.size(0))])\n",
    "#         n_eos = torch.vstack([encoder_output_n[i][negative_eos[i]] for i in range(encoder_output_n.size(0))])     \n",
    "        \n",
    "#         # compute the loss\n",
    "#         triplet_margin_loss = F.triplet_margin_loss(a_eos, p_eos, n_eos, \n",
    "#                                                     margin=self.margin, p=self.p,\n",
    "#                                                     eps=self.eps, swap=self.swap, \n",
    "#                                                     reduction=self.reduction)        \n",
    "#         loss = MLM_output.loss + triplet_margin_loss         \n",
    "\n",
    "        return maskedSent_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Early Stopping\n",
    "##### Track validation loss to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping(object):\n",
    "    def __init__(self, mode='min', min_delta=0, patience=10, percentage=False):\n",
    "        self.mode = mode\n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "        self.best = None\n",
    "        self.num_bad_epochs = 0\n",
    "        self.is_better = None\n",
    "        self._init_is_better(mode, min_delta, percentage)\n",
    "\n",
    "        if patience == 0:\n",
    "            self.is_better = lambda a, b: True\n",
    "            self.step = lambda a: False\n",
    "\n",
    "    def step(self, metrics):\n",
    "        if self.best is None:\n",
    "            self.best = metrics\n",
    "            return False\n",
    "\n",
    "        if torch.isnan(metrics):\n",
    "            return True\n",
    "\n",
    "        if self.is_better(metrics, self.best):\n",
    "            self.num_bad_epochs = 0\n",
    "            self.best = metrics\n",
    "        else:\n",
    "            self.num_bad_epochs += 1\n",
    "\n",
    "        if self.num_bad_epochs >= self.patience:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _init_is_better(self, mode, min_delta, percentage):\n",
    "        if mode not in {'min', 'max'}:\n",
    "            raise ValueError('mode ' + mode + ' is unknown!')\n",
    "        if not percentage:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best - min_delta\n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best + min_delta\n",
    "        else:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best - (\n",
    "                            best * min_delta / 100)\n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best + (\n",
    "                            best * min_delta / 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Training Logic\n",
    "##### Track gradients with wandb.watch and everything else with wandb.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_val(model, train_loader, test_loader, optimizer, es, config):\n",
    "    # Tell wandb to watch what the model gets up to: gradients, weights, and more!\n",
    "    wandb.watch(model, log=\"all\", log_freq=10)\n",
    "\n",
    "    # Run training and track with wandb\n",
    "    total_batches = len(train_loader) * config.epochs\n",
    "    print('num_training_steps', total_batches)\n",
    "    progress_bar = tqdm(range(total_batches))\n",
    "#     lr_scheduler = get_scheduler(\n",
    "#         \"linear\",\n",
    "#         optimizer=optimizer,\n",
    "#         num_warmup_steps=500,\n",
    "#         num_training_steps=total_batches,\n",
    "#     )\n",
    "    batch_ct = 0\n",
    "    for epoch in range(config.epochs):\n",
    "        model.train()\n",
    "#         for _, graphdata_batch in enumerate(train_loader):\n",
    "        for _, subprocess_batch in enumerate(train_loader):  \n",
    "            loss = train_batch(subprocess_batch, model, optimizer, progress_bar)\n",
    "#             loss = train_batch(graphdata_batch, model, optimizer, lr_scheduler, progress_bar)\n",
    "            batch_ct += 1\n",
    "            # Report metrics every 25th batch\n",
    "            if ((batch_ct) % 25) == 0:\n",
    "                train_log(loss, batch_ct, epoch)\n",
    "\n",
    "        metric = test(model, test_loader, batch_ct, epoch)\n",
    "        if es.step(metric):\n",
    "            output_model = './models_maskedSent/maskedSent_{}_epoch.pth'.format(epoch+1)\n",
    "#             output_model = './models_TML/TML_{}_epoch.pth'.format(epoch+1)\n",
    "            save(model, optimizer, output_model)\n",
    "            break\n",
    "        if epoch == (config.epochs-1):\n",
    "            output_model = './models_maskedSent/maskedSent_{}_epoch.pth'.format(epoch+1)\n",
    "#             output_model = './models_TML/TML_{}_epoch.pth'.format(config.epochs)\n",
    "            save(model, optimizer, output_model)\n",
    "\n",
    "# train_batch(batch, model, optimizer, lr_scheduler, progress_bar)\n",
    "def train_batch(batch, model, optimizer, progress_bar):\n",
    "                                                                                    \n",
    "#     graphdata_items = []\n",
    "#     for item in batch:\n",
    "#         graphdata_items.append({k: v.to(device) for k, v in item.items()})\n",
    "    subprocess_item = {k: v.to(device) for k, v in batch.items()}\n",
    "    \n",
    "    # Forward pass ➡\n",
    "    # model(graphdata_items, subprocess_item)\n",
    "    loss = model(subprocess_item)                                        \n",
    "    \n",
    "    # Backward pass ⬅\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Step with optimizer and lr_scheduler\n",
    "    optimizer.step()\n",
    "#     lr_scheduler.step()\n",
    "    progress_bar.update(1)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, batch_num, epoch):\n",
    "    model.eval()\n",
    "\n",
    "    # Run the model on some test examples\n",
    "    with torch.no_grad():\n",
    "        loss = 0\n",
    "#         for _, graphdata_batch in enumerate(test_loader):\n",
    "        for _, subprocess_batch in enumerate(test_loader):\n",
    "#             graphdata_items = []\n",
    "#             for item in graphdata_batch:\n",
    "#                 graphdata_items.append({k: v.to(device) for k, v in item.items()})\n",
    "            subprocess_item = {k: v.to(device) for k, v in subprocess_batch.items()}\n",
    "            loss += model(subprocess_item)\n",
    "        \n",
    "        loss /= len(test_loader)\n",
    "        test_log(loss, batch_num, epoch)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "#     # Save the model in the exchangeable ONNX format\n",
    "#     torch.onnx.export(model, images, \"model.onnx\")\n",
    "#     wandb.save(\"model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_log(loss, batch_num, epoch):\n",
    "    # Where the magic happens\n",
    "    wandb.log({\"epoch\": epoch, \"loss\": loss}, step=batch_num)\n",
    "    print(f\"Loss after \" + str(batch_num).zfill(5) + f\" steps: {loss:.3f}\")\n",
    "\n",
    "def test_log(loss, batch_num, epoch):\n",
    "    wandb.log({\"val_loss\": loss})\n",
    "    print(f\"Validation Loss after \" + str(batch_num).zfill(5) + f\" training steps: {loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model, optimizer, output_model):\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, output_model)\n",
    "\n",
    "def load(output_model):\n",
    "    checkpoint = torch.load(output_model)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/wandb/run-20220321_072249-5mval7vb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/yenting-thesis/thesis-TML-maskedSent/runs/5mval7vb\" target=\"_blank\">sage-aardvark-35</a></strong> to <a href=\"https://wandb.ai/yenting-thesis/thesis-TML-maskedSent\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': torch.Size([2, 849]), 'attention_mask': torch.Size([2, 849]), 'labels': torch.Size([2, 88])}\n",
      "num_training_steps 17060\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "641dac8e32d74c5da7173ae44d569c80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=17060.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 00025 steps: 11.063\n",
      "Loss after 00050 steps: 11.186\n",
      "Loss after 00075 steps: 10.681\n",
      "Loss after 00100 steps: 10.317\n",
      "Loss after 00125 steps: 9.836\n",
      "Loss after 00150 steps: 9.006\n",
      "Loss after 00175 steps: 7.937\n",
      "Loss after 00200 steps: 3.222\n",
      "Loss after 00225 steps: 0.636\n",
      "Loss after 00250 steps: 0.327\n",
      "Loss after 00275 steps: 0.314\n",
      "Loss after 00300 steps: 0.495\n",
      "Loss after 00325 steps: 0.351\n",
      "Loss after 00350 steps: 1.250\n",
      "Loss after 00375 steps: 0.347\n",
      "Loss after 00400 steps: 0.308\n",
      "Loss after 00425 steps: 1.126\n",
      "Loss after 00450 steps: 0.551\n",
      "Loss after 00475 steps: 0.238\n",
      "Loss after 00500 steps: 0.326\n",
      "Loss after 00525 steps: 0.192\n",
      "Loss after 00550 steps: 0.209\n",
      "Loss after 00575 steps: 0.201\n",
      "Loss after 00600 steps: 0.321\n",
      "Loss after 00625 steps: 0.257\n",
      "Loss after 00650 steps: 0.253\n",
      "Loss after 00675 steps: 0.526\n",
      "Loss after 00700 steps: 0.647\n",
      "Loss after 00725 steps: 0.164\n",
      "Loss after 00750 steps: 0.395\n",
      "Loss after 00775 steps: 1.297\n",
      "Loss after 00800 steps: 0.170\n",
      "Loss after 00825 steps: 0.070\n",
      "Loss after 00850 steps: 0.697\n",
      "Validation Loss after 00853 training steps: 0.268\n",
      "Loss after 00875 steps: 0.082\n",
      "Loss after 00900 steps: 0.282\n",
      "Loss after 00925 steps: 0.345\n",
      "Loss after 00950 steps: 0.753\n",
      "Loss after 00975 steps: 0.197\n",
      "Loss after 01000 steps: 0.451\n",
      "Loss after 01025 steps: 0.259\n",
      "Loss after 01050 steps: 0.236\n",
      "Loss after 01075 steps: 0.308\n",
      "Loss after 01100 steps: 0.833\n",
      "Loss after 01125 steps: 0.209\n",
      "Loss after 01150 steps: 0.569\n",
      "Loss after 01175 steps: 0.188\n",
      "Loss after 01200 steps: 0.220\n",
      "Loss after 01225 steps: 0.079\n",
      "Loss after 01250 steps: 0.077\n",
      "Loss after 01275 steps: 0.354\n",
      "Loss after 01300 steps: 0.084\n",
      "Loss after 01325 steps: 0.711\n",
      "Loss after 01350 steps: 0.467\n",
      "Loss after 01375 steps: 0.282\n",
      "Loss after 01400 steps: 0.193\n",
      "Loss after 01425 steps: 0.195\n",
      "Loss after 01450 steps: 0.099\n",
      "Loss after 01475 steps: 0.139\n",
      "Loss after 01500 steps: 0.611\n",
      "Loss after 01525 steps: 0.216\n",
      "Loss after 01550 steps: 0.109\n"
     ]
    }
   ],
   "source": [
    "# Build, train and analyze the model with the pipeline\n",
    "model = model_pipeline(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model with whole dataset once epoch number is set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

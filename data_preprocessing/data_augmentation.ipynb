{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06821505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.flow.sequential as naf\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644ed409",
   "metadata": {},
   "source": [
    "### Data Augmentation - En-De Round-trip Translation - Top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace8660d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/pytorch/fairseq.git fairseq-git\n",
    "!cd fairseq-git\n",
    "!pip3 install sacrebleu\n",
    "!pip3 install --editable .\n",
    "# !python setup.py build develop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02489dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install bitarray fastBPE hydra-core omegaconf regex requests sacremoses subword_nmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a60ac8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a33ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.getcwd()\n",
    "os.chdir('/content/fairseq')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "685bc2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./new_train_test_labeled_dataset.json','r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a48629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "en2de = torch.hub.load('pytorch/fairseq', 'transformer.wmt19.en-de.single_model', tokenizer='moses', bpe='fastbpe')\n",
    "de2en = torch.hub.load('pytorch/fairseq', 'transformer.wmt19.de-en.single_model', tokenizer='moses', bpe='fastbpe')\n",
    "en2de.cuda()\n",
    "de2en.cuda()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7492cf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_trip_translation(sent):\n",
    "    ende_rt_translation = []\n",
    "    en_toks = en2de.tokenize(sent)\n",
    "    en_bpe = en2de.apply_bpe(en_toks)\n",
    "    en_bin = en2de.binarize(en_bpe)\n",
    "    de_bin = en2de.generate(en_bin, beam=10, sampling=True, sampling_topk=10)\n",
    "    for b in de_bin:\n",
    "        de_sample = b['tokens']\n",
    "        de_bpe = en2de.string(de_sample)\n",
    "        de_toks = en2de.remove_bpe(de_bpe)\n",
    "        de = en2de.detokenize(de_toks)\n",
    "        trans = de2en.translate(de)\n",
    "        ende_rt_translation.append(trans)\n",
    "    return ende_rt_translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2947a86b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "356"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['document_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ba48bcf8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/var/folders/wn/y90bwflx3dd_bf70j8_n0z_r0000gn/T/ipykernel_96548/407971469.py\u001b[0m in \u001b[0;36mround_trip_translation\u001b[0;34m(sent)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0men_bpe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0men2de\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_bpe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_toks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0men_bin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0men2de\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_bpe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mde_bin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0men2de\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_bin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_topk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mde_bin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mde_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/torch/hub/pytorch_fairseq_main/fairseq/hub_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, tokenized_sentences, beam, verbose, skip_invalid_size_inputs, inference_step_args, prefix_allowed_tokens_fn, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m     ) -> List[List[Dict[str, torch.Tensor]]]:\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_sentences\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtokenized_sentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             return self.generate(\n\u001b[0m\u001b[1;32m    170\u001b[0m                 \u001b[0mtokenized_sentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             )[0]\n",
      "\u001b[0;32m~/.cache/torch/hub/pytorch_fairseq_main/fairseq/hub_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, tokenized_sentences, beam, verbose, skip_invalid_size_inputs, inference_step_args, prefix_allowed_tokens_fn, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_invalid_size_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_to_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             translations = self.task.inference_step(\n\u001b[0m\u001b[1;32m    190\u001b[0m                 \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minference_step_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             )\n",
      "\u001b[0;32m~/.cache/torch/hub/pytorch_fairseq_main/fairseq/tasks/fairseq_task.py\u001b[0m in \u001b[0;36minference_step\u001b[0;34m(self, generator, models, sample, prefix_tokens, constraints)\u001b[0m\n\u001b[1;32m    535\u001b[0m     ):\n\u001b[1;32m    536\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m             return generator.generate(\n\u001b[0m\u001b[1;32m    538\u001b[0m                 \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefix_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m             )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/torch/hub/pytorch_fairseq_main/fairseq/sequence_generator.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, models, sample, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \"\"\"\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     def _generate(\n",
      "\u001b[0;32m~/.cache/torch/hub/pytorch_fairseq_main/fairseq/sequence_generator.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, sample, prefix_tokens, constraints, bos_token)\u001b[0m\n\u001b[1;32m    337\u001b[0m                 \u001b[0;34m\"EnsembleModel: forward_decoder\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             ):\n\u001b[0;32m--> 339\u001b[0;31m                 lprobs, avg_attn_scores = self.model.forward_decoder(\n\u001b[0m\u001b[1;32m    340\u001b[0m                     \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                     \u001b[0mencoder_outs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/torch/hub/pytorch_fairseq_main/fairseq/sequence_generator.py\u001b[0m in \u001b[0;36mforward_decoder\u001b[0;34m(self, tokens, encoder_outs, incremental_states, temperature)\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# decode each model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_incremental_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 793\u001b[0;31m                 decoder_out = model.decoder.forward(\n\u001b[0m\u001b[1;32m    794\u001b[0m                     \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m                     \u001b[0mencoder_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/transformer/transformer_decoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, prev_output_tokens, encoder_out, incremental_state, features_only, full_context_alignment, alignment_layer, alignment_heads, src_lengths, return_all_hiddens)\u001b[0m\n\u001b[1;32m    215\u001b[0m         \"\"\"\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         x, extra = self.extract_features(\n\u001b[0m\u001b[1;32m    218\u001b[0m             \u001b[0mprev_output_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mencoder_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/transformer/transformer_decoder.py\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(self, prev_output_tokens, encoder_out, incremental_state, full_context_alignment, alignment_layer, alignment_heads)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0malignment_heads\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     ):\n\u001b[0;32m--> 239\u001b[0;31m         return self.extract_features_scriptable(\n\u001b[0m\u001b[1;32m    240\u001b[0m             \u001b[0mprev_output_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0mencoder_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/transformer/transformer_decoder.py\u001b[0m in \u001b[0;36mextract_features_scriptable\u001b[0;34m(self, prev_output_tokens, encoder_out, incremental_state, full_context_alignment, alignment_layer, alignment_heads)\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0mself_attn_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m             x, layer_attn, _ = layer(\n\u001b[0m\u001b[1;32m    342\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m                 \u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/torch/hub/pytorch_fairseq_main/fairseq/modules/transformer_layer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, encoder_out, encoder_padding_mask, incremental_state, prev_self_attn_state, prev_attn_state, self_attn_mask, self_attn_padding_mask, need_attn, need_head_weights)\u001b[0m\n\u001b[1;32m    506\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffn_layernorm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffn_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_resid\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "augmented_doc_train = []\n",
    "augmented_summ_train = []\n",
    "\n",
    "for i, (d, s) in enumerate(zip(data['document_train'], data['summary_train'])):\n",
    "    augmented_doc_train += round_trip_translation(d)\n",
    "    augmented_summ_train += round_trip_translation(s)\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3ff8e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3316f2e",
   "metadata": {},
   "source": [
    "### Back Translation - TopK\n",
    "\n",
    "##### Most suitable for process data as it doesn't change the structure\n",
    "##### Document Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c39bf3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./final_new_labeled_dataset.json','r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6558cacf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/I543118/.cache/torch/hub/pytorch_fairseq_main\n",
      "Loading codes from /Users/I543118/.cache/torch/pytorch_fairseq/81a0be5cbbf1c106320ef94681844d4594031c94c16b0475be11faa5a5120c48.63b093d59e7e0814ff799bb965ed4cbde30200b8c93a44bf8c1e5e98f5c54db3/bpecodes ...\n",
      "Read 30000 codes from the codes file.\n",
      "Using cache found in /Users/I543118/.cache/torch/hub/pytorch_fairseq_main\n",
      "100%|██████████████████████| 2992273886/2992273886 [04:05<00:00, 12188490.17B/s]\n",
      "Loading codes from /Users/I543118/.cache/torch/pytorch_fairseq/f42bb1b72d293668a5c50d9589fd2f3cc27322e390b1ef4cf3fdcf625c0d2fd7.bf6e22453272c2cba218a5ccecd045f73e926c34c1d66c47c9b31233343820a9/bpecodes ...\n",
      "Read 30000 codes from the codes file.\n"
     ]
    }
   ],
   "source": [
    "# Round-trip translations between English and German:\n",
    "en2de = torch.hub.load('pytorch/fairseq', 'transformer.wmt19.en-de.single_model', tokenizer='moses', bpe='fastbpe)\n",
    "de2en = torch.hub.load('pytorch/fairseq', 'transformer.wmt19.de-en.single_model', tokenizer='moses', bpe='fastbpe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eca1c38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/I543118/.cache/torch/hub/pytorch_fairseq_main\n",
      "100%|██████████████████████| 3037373614/3037373614 [03:35<00:00, 14106854.55B/s]\n",
      "Loading codes from /Users/I543118/.cache/torch/pytorch_fairseq/57b24ff1a88d5f14807352c9d8e17949d46ae3d0a06c1be3667929101011cdc0.8699fa0753ba14331352cc26ceb7aa46fba856bad52dd40fc97d3834418bf6f1/bpecodes ...\n",
      "Read 30000 codes from the codes file.\n",
      "Using cache found in /Users/I543118/.cache/torch/hub/pytorch_fairseq_main\n",
      "100%|██████████████████████| 3041223502/3041223502 [04:11<00:00, 12115315.41B/s]\n",
      "Loading codes from /Users/I543118/.cache/torch/pytorch_fairseq/f857729c1dcbbade3bbe70f7f58ede13e939a7b8278eb9015602e73be09e5cb4.ecb440f9c8cec1f28ee5023cdeab76f67eede4fc1ac0f2c70eeb2e65a25ad783/bpecodes ...\n",
      "Read 24000 codes from the codes file.\n"
     ]
    }
   ],
   "source": [
    "# Compare the results with English-Russian round-trip translation:\n",
    "en2ru = torch.hub.load('pytorch/fairseq', 'transformer.wmt19.en-ru.single_model', tokenizer='moses', bpe='fastbpe')\n",
    "ru2en = torch.hub.load('pytorch/fairseq', 'transformer.wmt19.ru-en.single_model', tokenizer='moses', bpe='fastbpe')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f8de5832",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/I543118/.cache/torch/hub/pytorch_fairseq_main\n",
      "Loading codes from /Users/I543118/.cache/torch/pytorch_fairseq/81a0be5cbbf1c106320ef94681844d4594031c94c16b0475be11faa5a5120c48.63b093d59e7e0814ff799bb965ed4cbde30200b8c93a44bf8c1e5e98f5c54db3/bpecodes ...\n",
      "Read 30000 codes from the codes file.\n"
     ]
    }
   ],
   "source": [
    "en2de = torch.hub.load('pytorch/fairseq', 'transformer.wmt19.en-de.single_model', tokenizer='moses', bpe='fastbpe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2225a38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually tokenize:\n",
    "en_toks = en2de.tokenize(data['document'][0])\n",
    "# Manually apply BPE:\n",
    "en_bpe = en2de.apply_bpe(en_toks)\n",
    "# Manually binarize:\n",
    "en_bin = en2de.binarize(en_bpe)\n",
    "\n",
    "# Generate five translations with top-k sampling:\n",
    "de_bin = en2de.generate(en_bin, beam=10, sampling=True, sampling_topk=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bc6a8de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(de_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5381d81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert one of the samples to a string and detokenize\n",
    "de_sample = de_bin[0]['tokens']\n",
    "de_bpe = en2de.string(de_sample)\n",
    "de_toks = en2de.remove_bpe(de_bpe)\n",
    "de = en2de.detokenize(de_toks)\n",
    "assert de == en2de.decode(de_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "38713c1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Application reception, extract and conversion gpa, check plagiarism of cover letter, read and evaluate cover letter, provisional ranking assigned, application marked as \"failed\"'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de2en.translate(de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dc29cc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.9 s, sys: 768 ms, total: 50.7 s\n",
      "Wall time: 50.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ende_rt_translation = []\n",
    "d = data['document'][0]\n",
    "en_toks = en2de.tokenize(d)\n",
    "en_bpe = en2de.apply_bpe(en_toks)\n",
    "en_bin = en2de.binarize(en_bpe)\n",
    "de_bin = en2de.generate(en_bin, beam=10, sampling=True, sampling_topk=10)\n",
    "for b in de_bin:\n",
    "    de_sample = b['tokens']\n",
    "    de_bpe = en2de.string(de_sample)\n",
    "    de_toks = en2de.remove_bpe(de_bpe)\n",
    "    de = en2de.detokenize(de_toks)\n",
    "    trans = de2en.translate(de)\n",
    "    ende_rt_translation.append(trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2f6a721e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ex 1: application received, extract and convert gpa, check plagiarism of motivation letter, read and score motivation letter, preliminary ranking assigned, application marked as \"failed\"\n",
      "\n",
      "Augmented:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Receive registration, extract and convert GPA, check plagiarism of cover letter, read and evaluate cover letter, assign preliminary ranking, mark application as \"failed\"',\n",
       " 'Application documents, extract and conversion of the gpa, plagiarism examination of the cover letter, reading and evaluating cover letters, provisional ranking assigned, cover letter marked \"failed\"',\n",
       " 'Submit application documents, extract and convert, check plagiarism of a cover letter, read and evaluate cover letter, assign provisional ranking, mark application as \"failed\"',\n",
       " 'Submit, extract and convert application documents, check cover letters for plagiarism protection, read and evaluate cover letters, assign provisional ranking, mark application as \"failure\"',\n",
       " 'Receive application documents, extract and implement GPA, check plagiarism from cover letters, read and evaluate cover letters, assign first ranking, mark application with \"failed\"',\n",
       " 'Receive application, if applicable extract and conversion, check the plagiarism of a cover letter, read and evaluate cover letter, provisional ranking assigned, application assessed as \"failed\"',\n",
       " 'Application entries, extractions and conversions gpas, verification of plagiarism in the cover letter, reading and values of the cover letter, preliminary ranking assigned, application marked \"failed\"',\n",
       " 'Receive applications, extract and conversion, check the plagiarism of cover letters and cover letters, pre-evaluation, application marked as \"failed\"',\n",
       " 'Receive application documents, extract from gpa, check the plagiarism in the cover letter, reading and scoring system, cover letter, ranking provisionally assigned, application marked as \"failed\"',\n",
       " \"Application deadline, excerpt, conversion, plagiarism test of a motivation writer, reading and evaluating a cover letter, assigning pre-grade, marking application as' failed '\"]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Ex 1: '+ data['document'][0] + '\\n\\nAugmented:')\n",
    "ende_rt_translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a2c1f4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "enru_rt_translation = []\n",
    "d = data['document'][0]\n",
    "en_toks = en2ru.tokenize(d)\n",
    "en_bpe = en2ru.apply_bpe(en_toks)\n",
    "en_bin = en2ru.binarize(en_bpe)\n",
    "ru_bin = en2ru.generate(en_bin, beam=10, sampling=True, sampling_topk=10)\n",
    "for b in ru_bin:\n",
    "    ru_sample = b['tokens']\n",
    "    ru_bpe = en2ru.string(ru_sample)\n",
    "    ru_toks = en2ru.remove_bpe(ru_bpe)\n",
    "    ru = en2ru.detokenize(ru_toks)\n",
    "    trans = ru2en.translate(ru)\n",
    "    enru_rt_translation.append(trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "76f31c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ex 1: application received, extract and convert gpa, check plagiarism of motivation letter, read and score motivation letter, preliminary ranking assigned, application marked as \"failed\"\n",
      "\n",
      "Augmented:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['application received, gpa extrapolation and conversion, verification for plagiarism of motivational letter, reading and evaluation of motivational letter, assignment of preliminary rating, application marked \"failed\"',\n",
       " 'application received, extrapolated and converted to gpa, plagiarism of motivational letter checked, incentive letter read and evaluated, provisional rating assigned, application marked as \"failed\"',\n",
       " 'application to receive, extrapolate and convert gpa, check for plagiarism of motivational letter, view and evaluate motivational letter, assign preliminary rating, mark application as \"rejected\"',\n",
       " 'received the application, extrapolation and conversion of the gp, verification of the plagiarism of the motivational letter, reading and evaluation of the motivational letter, assigning a preliminary rating, the application is marked as \"failed\"',\n",
       " 'application received, extrapolated and converted to gpa, check the plagiarism of the motivational letter, read and evaluate the motivational letter, assign a preliminary ranking, the application is marked as \"not successful\"',\n",
       " 'Application received, extroversion and conversion of the gp, check the plagiarism of the motivational letter, read and evaluate the motivational letter, a preliminary ranking is assigned, the application is marked as \"not fulfilled\"',\n",
       " 'Application received, extrapolated and converted gpa, validated for plagiarism incentive letters, read and evaluated motivational letter, given provisional place, application marked as \"failed\"',\n",
       " 'received the application, extrapolated and converted gPA, checked for plagiarism of the motivational letter, read and calculated the motivational letter, given a preliminary assessment, the application marked as \"failed\"',\n",
       " 'Receiving application, extrapolating and converting GHG, checking motivation letter for plagiarism, reading and evaluating motivation letter, preliminary ranking, application marked \"not successful\"',\n",
       " 'received application, extrapolation, gpa conversion, test motivation for plagiarism, read motivational letter, estimate preliminary rating, application marked as \"not fulfilled\"']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Ex 1: '+ data['document'][0] + '\\n\\nAugmented:')\n",
    "enru_rt_translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b69020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e26fd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "back_translation_aug = naw.BackTranslationAug(\n",
    "    from_model_name='facebook/wmt19-en-de',\n",
    "    to_model_name='facebook/wmt19-de-en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05a292a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ex 1: application received, extract and convert gpa, check plagiarism of motivation letter, read and score motivation letter, preliminary ranking assigned, application marked as \"failed\"\n",
      "\n",
      "Augmented: Application received, extract and convert gpa, check for plagiarism of the motivation letter, read and evaluate the motivation letter, assign preliminary ranking, application marked as \"failed\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "augmented_data = back_translation_aug.augment(data['document'][0])\n",
    "print('Ex 1: '+ data['document'][0] + '\\n\\nAugmented: ' + augmented_data + '\\n')\n",
    "# will need to also lower case the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7881905b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ex 2: payment processed, draw up completion mail, wait for arrival of customer in studio, after 7 days , pick-up reminder sent , hand over printouts, pack and send printouts, dropbox link added to completion mail, voucher added to completion mail, completion mail drawn up\n",
      "\n",
      "Augmented: Payment processing, E-mail completion, Waiting for the customer to arrive in the studio, after 7 days, Collection of the reminder, Delivery of the printouts, Packaging and sending of the printouts, Dropbox link to the completion e-mail, Voucher for completion e-mail, Completion e-mail created\n",
      "\n"
     ]
    }
   ],
   "source": [
    "augmented_data = back_translation_aug.augment(data['document'][4])\n",
    "print('Ex 2: '+ data['document'][4] + '\\n\\nAugmented: ' + augmented_data + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdc485d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ex 3: decide application offer, declination message, acceptation message, wait for kul if higher ranked applicants decline offer, application ended\n",
      "\n",
      "Augmented: Application offer, Rejection message, Acceptance message, Waiting for Kul, if higher ranking applicants reject the offer, Application terminated\n",
      "\n"
     ]
    }
   ],
   "source": [
    "augmented_data = back_translation_aug.augment(data['document'][3])\n",
    "print('Ex 3: '+ data['document'][3] + '\\n\\nAugmented: ' + augmented_data + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d8519c",
   "metadata": {},
   "source": [
    "##### Summary Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bbb1600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ex 1: assign preliminary ranking for incoming application\n",
      "\n",
      "Augmented : Determine preliminary order of priority for incoming applications\n",
      "\n"
     ]
    }
   ],
   "source": [
    "augmented_data = back_translation_aug.augment(data['summary'][0])\n",
    "print('Ex 1: '+ data['summary'][0] + '\\n\\nAugmented : ' + augmented_data + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb86c504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ex 1: finish order\n",
      "\n",
      "Augmented : \n",
      "\n"
     ]
    }
   ],
   "source": [
    "augmented_data = back_translation_aug.augment(data['summary'][4])\n",
    "print('Ex 2: '+ data['summary'][4] + '\\n\\nAugmented : ' + augmented_data + '\\n')\n",
    "# for None outcome -> use original or try other translation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7d44be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ex 1: decide application offer\n",
      "\n",
      "Augmented : Decide on the offer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "augmented_data = back_translation_aug.augment(data['summary'][3])\n",
    "print('Ex 3: '+ data['summary'][3] + '\\n\\nAugmented : ' + augmented_data + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20838133",
   "metadata": {},
   "source": [
    "### Sentence Level Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de61ced",
   "metadata": {},
   "source": [
    "#### Abstractive Summary Augmentation\n",
    "##### Document Augmentation - can set threshold, num of document sentences >= i.e. 5, can apply abstractive summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e85283f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "abst_summ_aug = nas.AbstSummAug(model_path='t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2253a334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ex 1: application received, extract and convert gpa, check plagiarism of motivation letter, read and score motivation letter, preliminary ranking assigned, application marked as \"failed\"\n",
      "\n",
      "Augmented: application received, extract and convert gpa, check plagiarism of motivation letter.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "augmented_data = abst_summ_aug.augment(data['document'][0])\n",
    "print('Ex 1: '+ data['document'][0] + '\\n\\nAugmented: ' + augmented_data + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "277c4f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ex 1: payment processed, draw up completion mail, wait for arrival of customer in studio, after 7 days , pick-up reminder sent , hand over printouts, pack and send printouts, dropbox link added to completion mail, voucher added to completion mail, completion mail drawn up\n",
      "\n",
      "Augmented: payment processed, draw up completion mail, wait for arrival of customer in studio. after 7 days. pick-up reminder sent, hand over printouts, pack and send printout. dropbox link added to completion mail and voucher added.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "augmented_data = abst_summ_aug.augment(data['document'][4])\n",
    "print('Ex 2: '+ data['document'][4] + '\\n\\nAugmented: ' + augmented_data + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f810038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ex 1: decide application offer, declination message, acceptation message, wait for kul if higher ranked applicants decline offer, application ended\n",
      "\n",
      "Augmented: kul: higher ranked applicants decline offer, application ended. decision made, declination message, acceptance message, wait for kul if kul declines offer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "augmented_data = abst_summ_aug.augment(data['document'][3])\n",
    "print('Ex 3: '+ data['document'][3] + '\\n\\nAugmented: ' + augmented_data + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bee1219",
   "metadata": {},
   "source": [
    "##### Summary Augmentation - Not suitable for label part as the structure changes - Verb + Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a7cccc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "abst_summ_aug_len10 = nas.AbstSummAug(model_path='t5-base', max_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78371c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ex 1: assign preliminary ranking for incoming application\n",
      "\n",
      "Augmented: Applicants are ranked according to preliminary ranking. preliminary ranking is based on the number of applications received.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "augmented_data = abst_summ_aug_len10.augment(data['summary'][0])\n",
    "print('Ex 1: '+ data['summary'][0] + '\\n\\nAugmented: ' + augmented_data + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "552f5cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ex 1: finish order\n",
      "\n",
      "Augmented: finish order\n",
      "\n"
     ]
    }
   ],
   "source": [
    "augmented_data = abst_summ_aug_len10.augment(data['summary'][4])\n",
    "print('Ex 2: '+ data['summary'][4] + '\\n\\nAugmented: ' + augmented_data + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "294f85b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ex 1: decide application offer\n",
      "\n",
      "Augmented: : decide application offer offer:.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "augmented_data = abst_summ_aug_len10.augment(data['summary'][3])\n",
    "print('Ex 3: '+ data['summary'][3] + '\\n\\nAugmented: ' + augmented_data + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27b0fba",
   "metadata": {},
   "source": [
    "#### Sentence Insertion \n",
    "##### Not suitable as it changes the sentence structure - Train set distribution might change and deviate too much from the test set distribution\n",
    "##### Document Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0575e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_inset_aug = nas.ContextualWordEmbsForSentenceAug(model_path='xlnet-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cd3ed02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ex 1: application received, extract and convert gpa, check plagiarism of motivation letter, read and score motivation letter, preliminary ranking assigned, application marked as \"failed\"\n",
      "\n",
      "Augmented: application received, extract and convert gpa, check plagiarism of motivation letter, read and score motivation letter, preliminary ranking assigned, application marked as \"failed\" or no certification or credential for successful performance of the application.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "augmented_data = sent_inset_aug.augment(data['document'][0])\n",
    "print('Ex 1: '+ data['document'][0] + '\\n\\nAugmented: ' + augmented_data + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae9098c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ex 1: payment processed, draw up completion mail, wait for arrival of customer in studio, after 7 days , pick-up reminder sent , hand over printouts, pack and send printouts, dropbox link added to completion mail, voucher added to completion mail, completion mail drawn up\n",
      "\n",
      "Augmented: payment processed, draw up completion mail, wait for arrival of customer in studio, after 7 days , pick-up reminder sent , hand over printouts, pack and send printouts, dropbox link added to completion mail, voucher added to completion mail, completion mail drawn up .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "augmented_data = sent_inset_aug.augment(data['document'][4])\n",
    "print('Ex 2: '+ data['document'][4] + '\\n\\nAugmented: ' + augmented_data + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d472f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ex 1: decide application offer, declination message, acceptation message, wait for kul if higher ranked applicants decline offer, application ended\n",
      "\n",
      "Augmented: decide application offer, declination message, acceptation message, wait for kul if higher ranked applicants decline offer, application ended after 45 hours after 60 applications were filled by 35% on 7 days after 20 days at this company and in other business at a wide ranging level for\n",
      "\n"
     ]
    }
   ],
   "source": [
    "augmented_data = sent_inset_aug.augment(data['document'][3])\n",
    "print('Ex 3: '+ data['document'][3] + '\\n\\nAugmented: ' + augmented_data + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed806f5",
   "metadata": {},
   "source": [
    "##### Summary Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e04b917d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ex 1: assign preliminary ranking for incoming application\n",
      "\n",
      "Augmented: assign preliminary ranking for incoming application category after 48 Days.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "augmented_data = sent_inset_aug.augment(data['summary'][0])\n",
    "print('Ex 1: '+ data['summary'][0] + '\\n\\nAugmented: ' + augmented_data + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b79addd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ex 2: finish order\n",
      "\n",
      "Augmented: finish order is only very important all through a good cooking experience if well crafted to create the perfect version or even your dinner if perfectly finished once.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "augmented_data = sent_inset_aug.augment(data['summary'][4])\n",
    "print('Ex 2: '+ data['summary'][4] + '\\n\\nAugmented: ' + augmented_data + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e56aa23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ex 3: decide application offer\n",
      "\n",
      "Augmented: decide application offer is too generous is our choice?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "augmented_data = sent_inset_aug.augment(data['summary'][3])\n",
    "print('Ex 3: '+ data['summary'][3] + '\\n\\nAugmented: ' + augmented_data + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504113ff",
   "metadata": {},
   "source": [
    "### Word Level Augmentation\n",
    "#### Synonym Replacement & Random Deletion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bccc4a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_syn = naw.SynonymAug(aug_src='wordnet', model_path=None, name='Synonym_Aug', aug_min=1, aug_max=10, aug_p=0.3, lang='eng', \n",
    "                     stopwords=None, tokenizer=None, reverse_tokenizer=None, stopwords_regex=None, force_reload=False, \n",
    "                     verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e5575ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPK=20 #default=100\n",
    "ACT = 'insert' #\"substitute\"\n",
    "aug_bert = naw.ContextualWordEmbsAug(\n",
    "    model_path='distilbert-base-uncased', \n",
    "    #device='cuda',\n",
    "    action=ACT, top_k=TOPK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef62cceb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aug_w2v = naw.WordEmbsAug(\n",
    "    model_type='glove', model_path='./glove/glove.6B.300d.txt',\n",
    "    action=\"substitute\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c897f948",
   "metadata": {},
   "source": [
    "### Flow Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03dcca18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'decide application offer, declination message, acceptation message, wait for kul if higher ranked applicants decline offer, application ended'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['document'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6591f386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['appoint application only, seek declination message, plea adaina message, wait subsequent for kul validation fact higher eight ranked visa applicants decline the offer, application service opening',\n",
       " 'decide application options, declination acceptance message, acceptation message, just application for kul examination again rank higher others sixth applicants supported decline any offer, entry yet 1997',\n",
       " 'if decide integrated ask, necessarily declination probably message, internationality message, wait wait application for solutions kul if two higher ranked applicants decline admittance offer, apply ended',\n",
       " 'try decide technology seeking, declination take wish, acceptation wait message, wait deadline two licenses kul if below ranked applicants simply weaker to buy, application offers ended',\n",
       " 'decision application server offer, choose declination status text, sekirei message, select wait application for kayn if sales ranked ineligible must reversing offer, decides application congregated ended',\n",
       " 'cannot decide application invitation offer, equality declination rejection message, acceptation message, validity until make kul again prices top seventh applicants january decline their want, application ended',\n",
       " 'give decide concerned application offer, declination message, acceptation preferred you, and wait application for connetquot analytical if higher grade ranked dole recipients deterioration offer, application ended',\n",
       " \"should application acceptance options, declination understand, acceptation always, wait procedure for taking dehsh application 'll many higher ranked test applicants decline interbk, quit application answer ended\",\n",
       " 'applicants might over application offer, declination rejection message, ryryryryryry message, request wait button for delton licenses if rising 38th colleges decline requests offer, permits submission ended',\n",
       " '“ decide keep request offer, inclusion declination need, acceptation validation convey, why for applicant kul longer higher ranked job applicants help decline offer, ask application 1999']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug = naf.Sequential([\n",
    "    aug_bert,aug_w2v\n",
    "])\n",
    "\n",
    "aug.augment(data['document'][3], n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f00a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flow_aug = naf.Sequential([\n",
    "#     back_translation_aug, abst_summ_aug\n",
    "# ])\n",
    "\n",
    "# flow_aug.augment(data['document'][3], n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516ddb2e",
   "metadata": {},
   "source": [
    "### Augmentation Strategy\n",
    "##### whether validation set is needed to decide 1:n and augmentation combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5a96ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python397jvsc74a57bd038b189b07233b7979ce6a7b2f19d07eb3f1cf3ce520727c8dd4113cd02bb1c25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

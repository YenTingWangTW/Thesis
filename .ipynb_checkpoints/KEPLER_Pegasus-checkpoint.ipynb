{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b80bd435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from transformers import PegasusModel, PegasusForConditionalGeneration, PegasusTokenizerFast, AdamW, get_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c62827",
   "metadata": {},
   "source": [
    "##### Load Train Dataset, Tokenizer & Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1449b0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./train_dataset.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4d6255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'google/pegasus-large'\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = PegasusTokenizerFast.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name, return_dict=True).to(torch_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b24fbd4",
   "metadata": {},
   "source": [
    "##### Define Custom Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6760383c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubprocessDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels['input_ids'][idx]) # torch.tensor(self.labels[idx])\n",
    "        return item # input_ids, attention_mask, labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels['input_ids']) # len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19571ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        # 1 training step - see if decoder weights change\n",
    "        item['labels'] = torch.tensor(self.encodings['input_ids'][idx])\n",
    "        return item # input_ids, attention_mask, labels\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids']) # len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa255cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(texts, labels):\n",
    "        encodings = tokenizer(texts, truncation=True, padding=True)\n",
    "        decodings = tokenizer(labels, truncation=True, padding=True)\n",
    "        graphdata_tokenized = GraphDataset(encodings)\n",
    "        subprocess_tokenized = SubprocessDataset(encodings, decodings)\n",
    "        return graphdata_tokenized, subprocess_tokenized\n",
    "\n",
    "graphdata_train_dataset, subprocess_train_dataset = tokenize_data(data['document'][:9], data['summary'][:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdcc0095",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': torch.Size([3, 77]), 'attention_mask': torch.Size([3, 77]), 'labels': torch.Size([3, 77])}\n",
      "{'input_ids': torch.Size([3, 77]), 'attention_mask': torch.Size([3, 77]), 'labels': torch.Size([3, 24])}\n"
     ]
    }
   ],
   "source": [
    "graphdata_train_dataloader = DataLoader(\n",
    "    graphdata_train_dataset, shuffle=False, batch_size=3\n",
    "#     num_workers=4\n",
    ")\n",
    "subprocess_train_dataloader = DataLoader(\n",
    "    subprocess_train_dataset, shuffle=True, batch_size=3\n",
    "#     num_workers=4\n",
    ")\n",
    "\n",
    "for graphdata_batch, subprocess_batch in zip(graphdata_train_dataloader, subprocess_train_dataloader):\n",
    "    break\n",
    "print({k: v.shape for k, v in graphdata_batch.items()})\n",
    "print({k: v.shape for k, v in subprocess_batch.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "674f5194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 am,<mask_1>, In system, Not in system, Record Vendor master,<mask_1>, Send to outbox, Send to priority outbox, Urgent</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "decoded_string = tokenizer.decode(graphdata_batch['input_ids'][0])\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a31b7fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  377,   346,   108,     2,   110,   108,   222,   327,   108,  1089,\n",
       "          115,   327,   108, 10297, 26658,  2080,   110,   108,     2,   110,\n",
       "          108,  8462,   112,   165,  4835,   108,  8462,   112,  3559,   165,\n",
       "         4835,   108, 42298,     1,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphdata_batch['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f5817f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([33, 42, 66])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_eos_idx(batch):\n",
    "    for input_ids in batch['input_ids']:\n",
    "        eos_id = input_ids == 1\n",
    "        idx = eos_id.nonzero()[0]\n",
    "        if 'eos_idx' in locals():\n",
    "            eos_idx = torch.cat((eos_idx, idx), 0)\n",
    "        else:\n",
    "            eos_idx = eos_id.nonzero()[0]\n",
    "    return eos_idx\n",
    "get_eos_idx(graphdata_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18318cee",
   "metadata": {},
   "source": [
    "##### Extract Model Output - encoder_last_hidden_state & loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5257472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.3893, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(**batch)\n",
    "output.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bf397a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.4870e-02, -4.7812e-02,  1.0340e-01,  ...,  4.9686e-02,\n",
       "          -5.3139e-02, -3.3579e-02],\n",
       "         [-1.5876e-02,  8.8630e-02, -1.5995e-02,  ...,  2.4677e-01,\n",
       "          -2.0305e-01, -5.9902e-03],\n",
       "         [-2.2632e-02,  1.6212e-03,  5.9650e-02,  ..., -7.6430e-02,\n",
       "           9.9388e-02, -3.3480e-02],\n",
       "         ...,\n",
       "         [ 1.0401e-01, -7.4977e-02,  5.6770e-02,  ...,  1.9212e-01,\n",
       "           2.0357e-01,  1.0390e-01],\n",
       "         [ 4.4121e-03, -1.8117e-02,  3.4443e-02,  ...,  1.6542e-01,\n",
       "           1.1914e-01,  2.3607e-01],\n",
       "         [ 2.3097e-02, -2.0389e-02,  2.1372e-02,  ...,  1.3392e-01,\n",
       "           1.5417e-01,  2.0417e-01]],\n",
       "\n",
       "        [[ 6.8591e-02, -8.6850e-02,  3.6081e-03,  ..., -3.4109e-02,\n",
       "          -8.6330e-02,  1.4675e-01],\n",
       "         [-1.7537e-02,  2.3032e-01,  2.4777e-02,  ..., -2.2457e-02,\n",
       "           3.6170e-02,  1.5164e-02],\n",
       "         [ 1.1273e-01, -1.7025e-01,  1.7403e-03,  ..., -3.4095e-02,\n",
       "          -2.1881e-02,  1.7849e-01],\n",
       "         ...,\n",
       "         [ 8.9253e-02,  5.6155e-02,  6.2068e-02,  ...,  4.7737e-02,\n",
       "          -4.5280e-02,  7.1844e-03],\n",
       "         [ 2.5462e-01,  1.3493e-01,  4.1192e-02,  ..., -1.1411e-01,\n",
       "          -2.5981e-01,  1.3338e-01],\n",
       "         [ 1.5304e-01,  5.6020e-04, -2.6973e-02,  ..., -2.1953e-01,\n",
       "          -3.9939e-02,  1.2318e-03]],\n",
       "\n",
       "        [[-2.9309e-01,  4.8449e-02, -2.2840e-02,  ..., -1.6829e-01,\n",
       "          -1.2873e-01, -8.3977e-02],\n",
       "         [-1.9948e-03,  1.1651e-01,  7.1636e-03,  ..., -1.8795e-02,\n",
       "          -2.9608e-01,  6.9001e-02],\n",
       "         [ 2.1443e-02, -4.2731e-02,  9.2712e-02,  ..., -2.0342e-01,\n",
       "           8.1062e-02,  8.3467e-02],\n",
       "         ...,\n",
       "         [ 8.4299e-02, -4.6478e-02,  3.9338e-02,  ...,  2.1715e-04,\n",
       "           1.2970e-01,  2.6049e-01],\n",
       "         [ 8.2510e-02, -8.6140e-02,  5.6906e-02,  ...,  3.4183e-02,\n",
       "          -5.4009e-03,  7.2573e-02],\n",
       "         [ 8.4036e-02, -4.0453e-02,  2.4501e-02,  ...,  3.8386e-02,\n",
       "          -2.4043e-02,  6.1068e-02]]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.encoder_last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f4ce643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 77, 1024])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.encoder_last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39b6c6a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eos_idx = get_eos_ids(batch)\n",
    "encoder_output = output.encoder_last_hidden_state\n",
    "encoder_output[0][eos_idx[0]].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37105c54",
   "metadata": {},
   "source": [
    "##### Try Model Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e1a3ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40d7b87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f8e4223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0641ddb84bf147b2972f4c049bb28cb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(torch_device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f95bc8c",
   "metadata": {},
   "source": [
    "### KEPLER Pegasus Model \n",
    "https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#TripletMarginLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "216b9099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class KeplerPegasusModel(nn.TripletMarginLoss):\n",
    "    \n",
    "    def __init__(self, model, margin: float = 1.0, p: float = 2., eps: float = 1e-6, swap: bool = False, size_average=None,\n",
    "                 reduce=None, reduction: str = 'mean'):\n",
    "        super().__init__(margin, p, eps, swap, size_average, reduce, reduction)\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self, graphdata_eos_idx, graphdata_batch, subprocess_batch):\n",
    "        # Triplet margin loss\n",
    "        model_output = self.model(**graphdata_batch)\n",
    "        encoder_output = model_output.encoder_last_hidden_state\n",
    "        anchor = encoder_output[0][graphdata_eos_idx[0]]\n",
    "        positive = encoder_output[1][graphdata_eos_idx[1]]\n",
    "        negative = encoder_output[2][graphdata_eos_idx[2]]\n",
    "        triplet_margin_loss = F.triplet_margin_loss(anchor, positive, negative, margin=self.margin, p=self.p,\n",
    "                                 eps=self.eps, swap=self.swap, reduction=self.reduction)\n",
    "        # Pegasus loss\n",
    "        MLM_output = self.model(**subprocess_batch)\n",
    "        \n",
    "        loss = MLM_output.loss + triplet_margin_loss \n",
    "        # monitor losses\n",
    "        # assertions?\n",
    "        \n",
    "        return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbaee261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(graphdata_train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b1674a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b520f507a2ac4f39a630d7b14606dca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for graphdata_batch, subprocess_batch in zip(graphdata_train_dataloader, subprocess_train_dataloader):        \n",
    "        graphdata_eos_idx = get_eos_idx(graphdata_batch)\n",
    "        graphdata_batch = {k: v.to(torch_device) for k, v in graphdata_batch.items()}\n",
    "        subprocess_batch = {k: v.to(torch_device) for k, v in subprocess_batch.items()}\n",
    "        kepler_pegasus_model = KeplerPegasusModel(model)\n",
    "        loss = kepler_pegasus_model(graphdata_eos_idx, graphdata_batch, subprocess_batch)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7af0222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking eos as sentence representation\n",
    "# use encodings itself (or random generated text) as labels, decoder_input_ids needed for graph data output...\n",
    "# create some sort of assertions? to make sure that the loss is calculated correctly\n",
    "# input data design questions (input data repetitions of subprocess side)\n",
    "# summarisation evaluation metrics questions\n",
    "\n",
    "# list of planned experiments - hypothesis vs. realities\n",
    "# thesis - experiment sections - data & sentence representation & KE loss definitions & evaluation metrics\n",
    "# github colab...\n",
    "# https://wandb.ai/site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4158f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd4e5bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeplerPegasusModel_test(nn.TripletMarginLoss):\n",
    "    \n",
    "    def __init__(self, model, margin: float = 1.0, p: float = 2., eps: float = 1e-6, swap: bool = False, size_average=None,\n",
    "                 reduce=None, reduction: str = 'mean'):\n",
    "        super().__init__(margin, p, eps, swap, size_average, reduce, reduction)\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self, anchor, positive, negative):\n",
    "        triplet_margin_loss = F.triplet_margin_loss(anchor, positive, negative, margin=self.margin, p=self.p,\n",
    "                                 eps=self.eps, swap=self.swap, reduction=self.reduction)       \n",
    "        return triplet_margin_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43f64f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = encoder_output[0][eos_idx[0]]\n",
    "positive = encoder_output[1][eos_idx[1]]\n",
    "negative = encoder_output[2][eos_idx[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c376cc81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0858, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_K = KeplerPegasusModel_test(model)\n",
    "loss_1 = model_K(anchor, positive, negative)\n",
    "loss_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0cf7d325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0858, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplet_margin_loss = nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "loss_2 = triplet_margin_loss(anchor, positive, negative)\n",
    "loss_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53cf27c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_1 == loss_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094b8fb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbfe57b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d39a5f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cde1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "# anchor = torch.randn(100, 128, requires_grad=True)\n",
    "# positive = torch.randn(100, 128, requires_grad=True)\n",
    "# negative = torch.randn(100, 128, requires_grad=True)\n",
    "# output = triplet_loss(anchor, positive, negative)\n",
    "# output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0ec9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# anchor_output = ...  # shape [None, 128]\n",
    "# positive_output = ...  # shape [None, 128]\n",
    "# negative_output = ...  # shape [None, 128]\n",
    "\n",
    "# d_pos = tf.reduce_sum(tf.square(anchor_output - positive_output), 1)\n",
    "# d_neg = tf.reduce_sum(tf.square(anchor_output - negative_output), 1)\n",
    "\n",
    "# loss = tf.maximum(0., margin + d_pos - d_neg)\n",
    "# loss = tf.reduce_mean(loss)\n",
    "\n",
    "# #         d_pos = torch.sum(torch.square(anchor_output - positive_output), -1)\n",
    "# #         d_neg = torch.sum(torch.square(anchor_output - negative_output), -1)\n",
    "# #         tripletloss = torch.maximum(0., margin + d_pos - d_neg)\n",
    "# #         loss = torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5130fe14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "93610300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SubprocessDataset(Dataset):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         data: pd.DataFrame,\n",
    "#         tokenizer: PegasusTokenizerFast\n",
    "#     ):                \n",
    "#         self.data = data\n",
    "#         self.tokenizer = tokenizer\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.data)  \n",
    "    \n",
    "#     def __getitem__(self, idx:int):\n",
    "#         data_row = self.data.iloc[idx]\n",
    "#         text = data_row['text']\n",
    "#         masked_sent = data_row['masked_sent']\n",
    "#         text_encoding = self.tokenizer(text, truncation=True, padding=True)\n",
    "#         masked_sent_encoding = self.tokenizer(text, truncation=True, padding=True)\n",
    "#         return dict(\n",
    "#             text=text,\n",
    "#             masked_sent=masked_sent,\n",
    "#             text_input_ids=text_encoding['input_ids'].flatten(),\n",
    "#             text_attetion_mask=text_encoding['attention_mask'].flatten(),\n",
    "#             labels = masked_sent_encoding['input_ids'].flatten()\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "1bd097b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pytorch_lightning as pl\n",
    "\n",
    "# class SubprocessDataModule(pl.LightningDataModule):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         train_df: pd.DataFrame,\n",
    "#         tokenizer: PegasusTokenizerFast,\n",
    "#         batch_size: int = 1\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.train_df = train_df\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.batch_size = batch_size\n",
    "        \n",
    "#     def setup(self, stage=None):\n",
    "#         self.train_dataset = SubprocessDataset(self.train_df, self.tokenizer)\n",
    "        \n",
    "#     def train_dataloader(self):\n",
    "#         return DataLoader(\n",
    "#             self.train_dataset,\n",
    "#             batch_size = self.batch_size,\n",
    "#             shuffle=True,\n",
    "#             num_workers=8\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c03013f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_epoch = 1\n",
    "# batch_size = 1\n",
    "# data_module = SubprocessDataModule(train_df, tokenizer, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e542d635",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b2b7076c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class KeplerPegasusModel(pl.LightningModule):\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.model = PegasusForConditionalGeneration.from_pretrained(model_name, return_dict=True).to(torch_device)\n",
    "        \n",
    "#     def forward(self, input_ids, attention_mask, labels):\n",
    "#         output = self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "#         return output.loss, output.logits\n",
    "    \n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         input_ids = batch['text_input_ids']\n",
    "#         attention_mask = batch['text_attetion_mask']\n",
    "#         labels = batch['labels']\n",
    "        \n",
    "#         loss, outputs = self(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "#         self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "#         return loss\n",
    "    \n",
    "#     def configure_optimizers(self):\n",
    "#         return AdamW(self.parameters(), lr=0.0001)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "52c03114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = KeplerPegasusModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "5d7f9b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "# from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir ./lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d4b640e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "# checkpoint_callback = ModelCheckpoint(\n",
    "#     dirpath = 'checkpoints',\n",
    "#     filename = 'best-checkpoint',\n",
    "#     save_top_k = 1,\n",
    "#     verbose = True,\n",
    "# #     monitor = 'val_loss',\n",
    "#     mode = 'min'\n",
    "# )\n",
    "\n",
    "# logger = TensorBoardLogger('lightning_logs', name='process-abstraction-pretrain')\n",
    "\n",
    "# trainer = pl.Trainer(\n",
    "#     logger=logger,\n",
    "#     enable_checkpointing=checkpoint_callback,\n",
    "#     max_epochs=n_epoch,\n",
    "#     gpus=0,\n",
    "#     progress_bar_refresh_rate=30,\n",
    "#     log_every_n_steps=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c9b6a093",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type                            | Params\n",
      "----------------------------------------------------------\n",
      "0 | model | PegasusForConditionalGeneration | 570 M \n",
      "----------------------------------------------------------\n",
      "568 M     Trainable params\n",
      "2.1 M     Non-trainable params\n",
      "570 M     Total params\n",
      "2,283.188 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1417aff96e941c789af35df3556ef14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/I543118/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/I543118/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'SubprocessDataset' on <module '__main__' (built-in)>\n",
      "/Users/I543118/opt/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "# trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9897c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e809a1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_texts = ['<mask_1>, Make sure there is no clash between social event and orientation sessions, Advertise social events for new students, Conduct the social event, Post event pictures in social network',\n",
    "#  '<mask_1>, Verify Sales Quote, Analyze RFQ and produce L&M estimates, Generate overhead costs and determine pricing options, Review Pricing Options and Generate Sales Quote, Sales Quote Delivered',\n",
    "#  '<mask_1>, <mask_1>, deny request, accept request, check security, accept request, accept request']\n",
    "\n",
    "# train_labels = ['Create social events',\n",
    "#  'RFQ Recieved',\n",
    "#  'check assigment rule , check credit rating']\n",
    "\n",
    "# subprocess_dataset = pd.DataFrame(list(zip(train_texts, train_labels)), columns = ['text', 'masked_sent'])\n",
    "# subprocess_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "236f946d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PegasusDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, encodings, labels):\n",
    "#         self.encodings = encodings\n",
    "#         self.labels = labels\n",
    "#     def __getitem__(self, idx):\n",
    "#         item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "#         item['labels'] = torch.tensor(self.labels['input_ids'][idx])  # torch.tensor(self.labels[idx])\n",
    "#         return item\n",
    "#     def __len__(self):\n",
    "#         return len(self.labels['input_ids'])  # len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e08b232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_data(texts, labels):\n",
    "#         encodings = tokenizer(texts, truncation=True, padding=True)\n",
    "#         decodings = tokenizer(labels, truncation=True, padding=True)\n",
    "#         dataset_tokenized = PegasusDataset(encodings, decodings)\n",
    "#         return dataset_tokenized\n",
    "\n",
    "# train_dataset = tokenize_data(train_texts, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9ecd4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataloader = DataLoader(\n",
    "#     train_dataset, shuffle=True, batch_size=2, collate_fn=DataCollator\n",
    "# )\n",
    "# for batch in train_dataloader:\n",
    "#     break\n",
    "# print({k: v.shape for k, v in batch.items()})\n",
    "# print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fe190a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
